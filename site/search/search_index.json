{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DIaL3 The University of Leicester hosts one of the four High Performance Computing (HPC) systems that forms the part of Distributed Research Utilising Advanced Computing (DiRAC). The other three sites are: Cambridge, Durham and Edinburgh. The DiRAC HPC resources are classified into 4 categories namely:- Data Intensive at Leicester (DIaL) Data Intensive at Cambridge Memory Intensive at Durham Extreme Scaling at Edinburgh For more information about DiRAC service, please see the link: https://dirac.ac.uk/ . Data Intensive at Leicester (DIaL) is a term collectively used for two HPC systems at UoL: DIaL2.5 and DIaL3. For more information about DIaL2.5, please follow DIaL2.5 (You will have to login using your UoL username and password). DIaL3 is a new HPC facility that has been recently purchased under the grant of 2 million pounds provided by Science and Technology Facilities Council (STFC). The system will be available for production usage from September 30, 2021. The following image shows the DIaL3 HPC facility at UoL. For more information about DIaL3 and its architecture, please visit the following links. Architecture FileSystem","title":"Home"},{"location":"#dial3","text":"The University of Leicester hosts one of the four High Performance Computing (HPC) systems that forms the part of Distributed Research Utilising Advanced Computing (DiRAC). The other three sites are: Cambridge, Durham and Edinburgh. The DiRAC HPC resources are classified into 4 categories namely:- Data Intensive at Leicester (DIaL) Data Intensive at Cambridge Memory Intensive at Durham Extreme Scaling at Edinburgh For more information about DiRAC service, please see the link: https://dirac.ac.uk/ . Data Intensive at Leicester (DIaL) is a term collectively used for two HPC systems at UoL: DIaL2.5 and DIaL3. For more information about DIaL2.5, please follow DIaL2.5 (You will have to login using your UoL username and password). DIaL3 is a new HPC facility that has been recently purchased under the grant of 2 million pounds provided by Science and Technology Facilities Council (STFC). The system will be available for production usage from September 30, 2021. The following image shows the DIaL3 HPC facility at UoL. For more information about DIaL3 and its architecture, please visit the following links. Architecture FileSystem","title":"DIaL3"},{"location":"About_dial3/architecture/","text":"DIaL3 Technical Specifications The DIaL3 system has a total of 25,600 AMD EPYC compute cores whic is equivalent to 57.6 Tera flops of computing power. The detailed specifications are as follows:- Compute nodes: The system comprise of 200 compute nodes each having 2 x AMD EPYC 7742 (Rome) processors. Each processor has 64 cores running at a base frequency of 2.25 GHz thereby leading to 128 cores per nodes and a total of 25,600 cores in total. Login nodes : There are two login nodes comprising of 2 x AMD EPYC 7502 processors. Each processor has 32-Cores thereby leading to a total of 128 cores on login nodes. RAM on each compute node: Each compute node consists of 512 GB of DDR4 RAM thereby providing 4 GB of RAM for each core. Interconnect: Infiniband ------ Storage: 3PB ----- Operating System: Centos (Version ---) Job Scheduler: Slurm Workload manager","title":"Architecture"},{"location":"About_dial3/architecture/#dial3-technical-specifications","text":"The DIaL3 system has a total of 25,600 AMD EPYC compute cores whic is equivalent to 57.6 Tera flops of computing power. The detailed specifications are as follows:- Compute nodes: The system comprise of 200 compute nodes each having 2 x AMD EPYC 7742 (Rome) processors. Each processor has 64 cores running at a base frequency of 2.25 GHz thereby leading to 128 cores per nodes and a total of 25,600 cores in total. Login nodes : There are two login nodes comprising of 2 x AMD EPYC 7502 processors. Each processor has 32-Cores thereby leading to a total of 128 cores on login nodes. RAM on each compute node: Each compute node consists of 512 GB of DDR4 RAM thereby providing 4 GB of RAM for each core. Interconnect: Infiniband ------ Storage: 3PB ----- Operating System: Centos (Version ---) Job Scheduler: Slurm Workload manager","title":"DIaL3 Technical Specifications"},{"location":"About_dial3/filesystem/","text":"File Systems on DIaL3 There are a number of file systems on DIaL3 that should be used in different ways: /home /scratch /data /tmp /home Every user has a home directory within the /home file system. This will be your current working directory when you login to DIaL3. The home directory should only be used to store program code, job submission scripts, configuration files and small amounts of data. Your home directory can be referenced on the Linux command line with the shorthand ~ . The simple command cd with no arguments will return you to your home directory. Users' home directories are backed up nightly. /scratch You will have a scratch directory on the scratch file system for each project that you are associated with. The location will be /scratch/project/username - so users in multiple projects will have several to choose from. As each user's scratch directory is readable by other members of the same project, it's important to choose the correct one depending on which project is being worked on. The directory /scratch/project/shared has special permissions to ensure that all files within are always owned by the project group. The scratch directory used should be the main location for job files, and generally should be used as the working directory for jobs. The scratch space has quotas applied which are in line with those requested for \u201cwork\u201d in DiRAC time applications. These are not allocations, only quotas, so can add up to more than the available storage. Warning: Files within /scratch are not backed up . Furthermore, there is an automated process which deletes any files that haven't been accessed in more than 60 days . There will be no prior warning that files will be deleted, it is up to users to ensure that important data is not kept in /scratch for long term storage. /data This file system is provided for medium term storage of results prior to publication and transfer of data to the users own institution. The structure of the filesystem is similar to /scratch . It is also not backed up and will be swept of old data. The data sweeping will be based on modification times being more than 9 months. /tmp Each compute node has a small amount of local disc mounted on /tmp . For some jobs there may be a performance gain over /tmp in using this file system for intermediate files. Standard compute nodes have 100GB available. The preferred way to use /tmp within a job is to refer to it using the environment variable TMPDIR . This way the job's files are cleaned-up when the job finishes automatically. Otherwise it is your job's responsibility to remove files from the local file system when it ends. Warning: The /tmp file system is not backed up, and the contents are deleted whenever the compute node reboots for any reason. Files on /tmp should only be considered safe for the duration of the job which they belong to.","title":"Filesystem"},{"location":"About_dial3/filesystem/#file-systems-on-dial3","text":"There are a number of file systems on DIaL3 that should be used in different ways: /home /scratch /data /tmp","title":"File Systems on DIaL3"},{"location":"About_dial3/filesystem/#home","text":"Every user has a home directory within the /home file system. This will be your current working directory when you login to DIaL3. The home directory should only be used to store program code, job submission scripts, configuration files and small amounts of data. Your home directory can be referenced on the Linux command line with the shorthand ~ . The simple command cd with no arguments will return you to your home directory. Users' home directories are backed up nightly.","title":"/home"},{"location":"About_dial3/filesystem/#scratch","text":"You will have a scratch directory on the scratch file system for each project that you are associated with. The location will be /scratch/project/username - so users in multiple projects will have several to choose from. As each user's scratch directory is readable by other members of the same project, it's important to choose the correct one depending on which project is being worked on. The directory /scratch/project/shared has special permissions to ensure that all files within are always owned by the project group. The scratch directory used should be the main location for job files, and generally should be used as the working directory for jobs. The scratch space has quotas applied which are in line with those requested for \u201cwork\u201d in DiRAC time applications. These are not allocations, only quotas, so can add up to more than the available storage. Warning: Files within /scratch are not backed up . Furthermore, there is an automated process which deletes any files that haven't been accessed in more than 60 days . There will be no prior warning that files will be deleted, it is up to users to ensure that important data is not kept in /scratch for long term storage.","title":"/scratch"},{"location":"About_dial3/filesystem/#data","text":"This file system is provided for medium term storage of results prior to publication and transfer of data to the users own institution. The structure of the filesystem is similar to /scratch . It is also not backed up and will be swept of old data. The data sweeping will be based on modification times being more than 9 months.","title":"/data"},{"location":"About_dial3/filesystem/#tmp","text":"Each compute node has a small amount of local disc mounted on /tmp . For some jobs there may be a performance gain over /tmp in using this file system for intermediate files. Standard compute nodes have 100GB available. The preferred way to use /tmp within a job is to refer to it using the environment variable TMPDIR . This way the job's files are cleaned-up when the job finishes automatically. Otherwise it is your job's responsibility to remove files from the local file system when it ends. Warning: The /tmp file system is not backed up, and the contents are deleted whenever the compute node reboots for any reason. Files on /tmp should only be considered safe for the duration of the job which they belong to.","title":"/tmp"},{"location":"Analyse_code/Debug_code/","text":"Under Construction","title":"Debug your Code"},{"location":"Analyse_code/Install_arm_forge/","text":"Install Arm forge on your PC You can install Arm forge on your local system and use the locally installed Arm forge to view your code profile. This method will be faster that X forwarding (as mentioned in What is Arm Forge ). Arm forge describes this way of working as \u201cReverse connect\u201d in which we generate the profiles offline (by usual job submission scripts) and instead of using -X flag to use a GUI, we install the software on our local system. Once we have the software on our local system, we connect it with the one present on the server (DIaL3 in this case). Please follow the steps below to install Arm forge and connect it with the server. (A). First download the Arm forge from the official website (https://developer.arm.com/tools-and-software/server-and-hpc/downloads/arm-forge) (B). Install the arm forge in your desired location. For Linux, the downloaded files contain a file called \u2018installer\u2019. Double clicking on it will install the software in your desired location. (C). Change to the directory where you installed the software. For example, I installed it in /home/my_preferred_location/arm/forge/21.0.3. $ cd /home/my_preferred_location/arm/forge/21.0.3. (D). . Change directory to the bin directory. $ cd /home/my_preferred_location/arm/forge/21.0.3. (E). . Launch the arm forge software. $ ./forge (F). Go to the remote launch option in the main GUI and click configure. (G). After clicking configure, you will be presented to add a new connection. Click on add and fill in the following values (Please adjust accordingly). Connection Name: Fill in any name you want (e.g. Dial3) Host name:- Enter the host name to which you want to connect to. Arm forge expects these values to be a space separated list. (For dial, you will only need to enter the first value i.e. ignore the value after space). Eg. :- dc-ragt1@dial.dirac.ac.uk dc-ragt1@d3-login02.rcs.le.ac.uk Remote Installation Directory:- /cm/shared/apps/arm/forge/21.0.2 (Please do not change this). (H). Click Ok, then close. Now you will see an entry with the Connection name (here Dial3) in the main GUI under remote launch. Please click on this name and you will be prompted to enter your password twice. First to connect with Dial2.5 and then to Dial3. Please note that this is a one time setup and you do not need to do it again. You will only need to connect with the server anytime you want to use the Arm forge.","title":"Install Arm Forge"},{"location":"Analyse_code/Install_arm_forge/#install-arm-forge-on-your-pc","text":"You can install Arm forge on your local system and use the locally installed Arm forge to view your code profile. This method will be faster that X forwarding (as mentioned in What is Arm Forge ). Arm forge describes this way of working as \u201cReverse connect\u201d in which we generate the profiles offline (by usual job submission scripts) and instead of using -X flag to use a GUI, we install the software on our local system. Once we have the software on our local system, we connect it with the one present on the server (DIaL3 in this case). Please follow the steps below to install Arm forge and connect it with the server. (A). First download the Arm forge from the official website (https://developer.arm.com/tools-and-software/server-and-hpc/downloads/arm-forge) (B). Install the arm forge in your desired location. For Linux, the downloaded files contain a file called \u2018installer\u2019. Double clicking on it will install the software in your desired location. (C). Change to the directory where you installed the software. For example, I installed it in /home/my_preferred_location/arm/forge/21.0.3. $ cd /home/my_preferred_location/arm/forge/21.0.3. (D). . Change directory to the bin directory. $ cd /home/my_preferred_location/arm/forge/21.0.3. (E). . Launch the arm forge software. $ ./forge (F). Go to the remote launch option in the main GUI and click configure. (G). After clicking configure, you will be presented to add a new connection. Click on add and fill in the following values (Please adjust accordingly). Connection Name: Fill in any name you want (e.g. Dial3) Host name:- Enter the host name to which you want to connect to. Arm forge expects these values to be a space separated list. (For dial, you will only need to enter the first value i.e. ignore the value after space). Eg. :- dc-ragt1@dial.dirac.ac.uk dc-ragt1@d3-login02.rcs.le.ac.uk Remote Installation Directory:- /cm/shared/apps/arm/forge/21.0.2 (Please do not change this). (H). Click Ok, then close. Now you will see an entry with the Connection name (here Dial3) in the main GUI under remote launch. Please click on this name and you will be prompted to enter your password twice. First to connect with Dial2.5 and then to Dial3. Please note that this is a one time setup and you do not need to do it again. You will only need to connect with the server anytime you want to use the Arm forge.","title":"Install Arm forge on your PC"},{"location":"Analyse_code/Perf_report/","text":"Arm Performance Reports ARM Performance Reports is a low overhead tool that provides a high-level overview of your application\u2019s performance including computation, communication, and I/O. This tool provides a one-page text or HTML summary about the application's performance. To use this tool, all you need to do is prefix your execution command with perf-report . Please be advised that you do not need to recompile your code with any additional flags for generating a performance report. Instead, use the flags which you believe gives the best performance for your code. You can use the following command for this purpose. #For a single core Job. $ perf-report <executable> #For a MPI job. $ perf-report mpirun -n 4 <executable> You can also submit a job to scheduler to automatically generate the performance report for you. You can use the following example script (for an Hybrid MPI + OpenMP job) to generate the performance report for your code. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path perf-report --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable This will generate a .html file and .txt file which will show the various details about your code such as total run time, time consumed in MPI, whether code is compute bound or I/O bound etc. You can open the .html file in any browser. A sample image of performance report is shown below for your reference.","title":"Performance Report"},{"location":"Analyse_code/Perf_report/#arm-performance-reports","text":"ARM Performance Reports is a low overhead tool that provides a high-level overview of your application\u2019s performance including computation, communication, and I/O. This tool provides a one-page text or HTML summary about the application's performance. To use this tool, all you need to do is prefix your execution command with perf-report . Please be advised that you do not need to recompile your code with any additional flags for generating a performance report. Instead, use the flags which you believe gives the best performance for your code. You can use the following command for this purpose. #For a single core Job. $ perf-report <executable> #For a MPI job. $ perf-report mpirun -n 4 <executable> You can also submit a job to scheduler to automatically generate the performance report for you. You can use the following example script (for an Hybrid MPI + OpenMP job) to generate the performance report for your code. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path perf-report --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable This will generate a .html file and .txt file which will show the various details about your code such as total run time, time consumed in MPI, whether code is compute bound or I/O bound etc. You can open the .html file in any browser. A sample image of performance report is shown below for your reference.","title":"Arm Performance Reports"},{"location":"Analyse_code/Profile_code/","text":"Profile your code using Arm MAP Arm MAP is a Arm Forge's scalable profiler that can be used to profile serial and parallel applications. It allows you to profile your code without distorting application behaviour. It can be used to profile C, C++, Cuda, Fortran and Python code with no instrumentation or code changes. It helps the developers to accelerate their code by revealing the bottlenecks in their code such as the function taking maximum time, excessive time spent in MPI communication, slow I/O etc. In order to profile your code, please follow the stpes given below. 1. Recompile your code by including the debug flag i.e. -g . This will allow the Arm MAP to gather the necessary information during runtime. 2. You can use the following script to the scheduler which will automatically generate your code profile. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path #The following command is responsible for generating your code profile. map --profile --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable 3. Once your job ends, you will see a file with an extension .map which is your code profile. This file would be placed in the same directory from which the job was launched. 4. Now connect the arm forge form your local system to the server and click on Arm map in the main GUI as shown in the image below. 5. Click the option 'load the profile data from a previous run\u2019. Browse to the location of file generated in Step3 . 6. This will now open the map profile and will give all necessary details about your code such as: OpenMp stacks, function times, MPI times, overhead etc. We can navigate through the source code also to see which function is taking more time. A typical code profile would like the image shown below.","title":"Profile your Code"},{"location":"Analyse_code/Profile_code/#profile-your-code-using-arm-map","text":"Arm MAP is a Arm Forge's scalable profiler that can be used to profile serial and parallel applications. It allows you to profile your code without distorting application behaviour. It can be used to profile C, C++, Cuda, Fortran and Python code with no instrumentation or code changes. It helps the developers to accelerate their code by revealing the bottlenecks in their code such as the function taking maximum time, excessive time spent in MPI communication, slow I/O etc. In order to profile your code, please follow the stpes given below. 1. Recompile your code by including the debug flag i.e. -g . This will allow the Arm MAP to gather the necessary information during runtime. 2. You can use the following script to the scheduler which will automatically generate your code profile. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path #The following command is responsible for generating your code profile. map --profile --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable 3. Once your job ends, you will see a file with an extension .map which is your code profile. This file would be placed in the same directory from which the job was launched. 4. Now connect the arm forge form your local system to the server and click on Arm map in the main GUI as shown in the image below. 5. Click the option 'load the profile data from a previous run\u2019. Browse to the location of file generated in Step3 . 6. This will now open the map profile and will give all necessary details about your code such as: OpenMp stacks, function times, MPI times, overhead etc. We can navigate through the source code also to see which function is taking more time. A typical code profile would like the image shown below.","title":"Profile your code using Arm MAP"},{"location":"Analyse_code/about_arm_forge/","text":"Arm Forge Arm forge is one of the leading HPC and server development tools used in research, academia and industry for high performance codes written either in C, C++, Fortran or Python. Arm forge has three major components:- Arm Performance Reports Arm MAP Arm DDT To use Arm forge on DiAL3, please load the Arm forge module by the following command. module load arm/forge/21.0.2 . You can use Arm forge in two different ways:- Using X forwarding (might be slow) By installing Arm forge on your local computer (No License needed). Use Arm Forge by X forwarding First login to DiAL3 cluster by using the following command (please pay attention to the -X flag in the command). $ ssh -X your_username@dial3.dirac.ac.uk Enter your password to login. Once you have logged in, load the Arm forge module by entering the following command in the terminal. $ module load arm/forge/21.0.2 To see what all paths and variables have been added to your environment by the above load command, type the following. $ module display arm/forge/21.0.2 This will show the following output on your screen. ---------------------------------------------------------------------------- /cm/shared/modulefiles/arm/forge/21.0.2: ---------------------------------------------------------------------------- whatis(\"Loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment \") prepend_path(\"PATH\",\"/cm/shared/apps/arm/forge/21.0.2/bin\") prepend_path(\"LIBRARY_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"LD_RUN_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"CPATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") setenv(\"ARM_LICENSE_DIR\",\"/cm/shared/apps/arm/forge/21.0.2/licences\") help([[ forge/21.0.2 - loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment ]]) If you see the first entry in the prepend path, you will see the location where all binaries of arm forge are located. Change to that directory to see what all binaries are there. $ cd /cm/shared/apps/arm/forge/21.0.2/bin #List the contents of that directory. $ ls allinea-client ddt-mpirun forge-mpirun map ddt forge forge-probe perf-report ddt-client forge-client make-profiler-libraries #Open the forge GUI by using the following command. $ ./forge Now you can use this GUI to debug your code or view profile reports . USE Arm forge by Installing on your PC You can also use Arm forge by first installing it on your PC (you do not need a license for that) and using that to view your code profile. For more details, please click Install Arm Forge .","title":"What is Arm Forge"},{"location":"Analyse_code/about_arm_forge/#arm-forge","text":"Arm forge is one of the leading HPC and server development tools used in research, academia and industry for high performance codes written either in C, C++, Fortran or Python. Arm forge has three major components:- Arm Performance Reports Arm MAP Arm DDT To use Arm forge on DiAL3, please load the Arm forge module by the following command. module load arm/forge/21.0.2 . You can use Arm forge in two different ways:- Using X forwarding (might be slow) By installing Arm forge on your local computer (No License needed).","title":"Arm Forge"},{"location":"Analyse_code/about_arm_forge/#use-arm-forge-by-x-forwarding","text":"First login to DiAL3 cluster by using the following command (please pay attention to the -X flag in the command). $ ssh -X your_username@dial3.dirac.ac.uk Enter your password to login. Once you have logged in, load the Arm forge module by entering the following command in the terminal. $ module load arm/forge/21.0.2 To see what all paths and variables have been added to your environment by the above load command, type the following. $ module display arm/forge/21.0.2 This will show the following output on your screen. ---------------------------------------------------------------------------- /cm/shared/modulefiles/arm/forge/21.0.2: ---------------------------------------------------------------------------- whatis(\"Loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment \") prepend_path(\"PATH\",\"/cm/shared/apps/arm/forge/21.0.2/bin\") prepend_path(\"LIBRARY_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"LD_RUN_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"CPATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") setenv(\"ARM_LICENSE_DIR\",\"/cm/shared/apps/arm/forge/21.0.2/licences\") help([[ forge/21.0.2 - loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment ]]) If you see the first entry in the prepend path, you will see the location where all binaries of arm forge are located. Change to that directory to see what all binaries are there. $ cd /cm/shared/apps/arm/forge/21.0.2/bin #List the contents of that directory. $ ls allinea-client ddt-mpirun forge-mpirun map ddt forge forge-probe perf-report ddt-client forge-client make-profiler-libraries #Open the forge GUI by using the following command. $ ./forge Now you can use this GUI to debug your code or view profile reports .","title":"Use Arm Forge by X forwarding"},{"location":"Analyse_code/about_arm_forge/#use-arm-forge-by-installing-on-your-pc","text":"You can also use Arm forge by first installing it on your PC (you do not need a license for that) and using that to view your code profile. For more details, please click Install Arm Forge .","title":"USE Arm forge by Installing on your PC"},{"location":"Getting_started/connecting_dial3/","text":"Connecting to DIaL3 Once you have requested acsess to DIaL3 , you can access the login nodes from an SSH client using either of the following address. dial3.dirac.ac.uk , d3-login01.rcs.le.ac.uk or d3-login02.rcs.le.ac.uk From your terminal type the following command: $ ssh your_username@d3-login02.rcs.le.ac.uk You will be prompted to enter your password. $ your_username@d3-login02.rcs.le.ac.uk's password: Once you enter your password, you will see the following message on your screen. Last login: Sat Sep 4 10:27:27 2021 from 444.222.111.88 # The above message shows the last time you logged in and the IP address from which you logged in.","title":"Connecting"},{"location":"Getting_started/connecting_dial3/#connecting-to-dial3","text":"Once you have requested acsess to DIaL3 , you can access the login nodes from an SSH client using either of the following address. dial3.dirac.ac.uk , d3-login01.rcs.le.ac.uk or d3-login02.rcs.le.ac.uk From your terminal type the following command: $ ssh your_username@d3-login02.rcs.le.ac.uk You will be prompted to enter your password. $ your_username@d3-login02.rcs.le.ac.uk's password: Once you enter your password, you will see the following message on your screen. Last login: Sat Sep 4 10:27:27 2021 from 444.222.111.88 # The above message shows the last time you logged in and the IP address from which you logged in.","title":"Connecting to DIaL3"},{"location":"Getting_started/create_account/","text":"Request Access In order to use DIaL3 system at the University of Leicester, you need to apply for an account. Access to the DIaL3 (part of DiRAC) service is through peer reviewed applications. There are details of how to request access on the DiRAC website. Once access has been approved, accounts are requested through the SAFE project and user management system operated by EPCC . SAFE stands for S ervice A dministration F rom E PCC. Every DiRAC user has an account on SAFE. You can find more information on the DiRAC website .","title":"Create Account"},{"location":"Getting_started/create_account/#request-access","text":"In order to use DIaL3 system at the University of Leicester, you need to apply for an account. Access to the DIaL3 (part of DiRAC) service is through peer reviewed applications. There are details of how to request access on the DiRAC website. Once access has been approved, accounts are requested through the SAFE project and user management system operated by EPCC . SAFE stands for S ervice A dministration F rom E PCC. Every DiRAC user has an account on SAFE. You can find more information on the DiRAC website .","title":"Request Access"},{"location":"Run_computation/Slurm_workload_manager/","text":"SLURM Workload Manager DiaL3 use SLURM as its workload manager i.e. SLURM is responsible for scheduling and running jobs on the compute nodes as and when they are available. Some of the useful SLURM commands are:- salloc :- is used to allocate resources for a job in real time. Typically this is used to allocate resources and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks. sbatch :- is used to submit a job script for later execution. The script will typically contain one or more srun commands to launch parallel tasks. scancel :- is used to cancel a pending or running job or job step. It can also be used to send an arbitrary signal to all processes associated with a running job or job step. scontrol :- is the administrative tool used to view and/or modify SLURM state. Note that many of the scontrol commands can only be executed as user root. sinfo :- reports the state of partitions and nodes managed by SLURM. It has a wide variety of filtering, sorting, and formatting options. squeue :- reports the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. By default, it reports the running jobs in priority order and then the pending jobs in priority order. srun :- is used to submit a job for execution or initiate job steps in real time. srun command has a wide variety of options to specify resource requirements, including: minimum and maximum node count, processor count, specific nodes to use or not use, and specific node characteristics (so much memory, disk space, certain required features, etc.). A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation. sbatch As mnetioned above, sbatch submits a batch script to SLURM. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with #SBATCH before any executable commands in the script. sbatch will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script. sbatch exits immediately after the script is successfully transferred to the SLURM controller and assigned a SLURM job ID. Example: #!/bin/bash # #! Example SLURM job script for DiRAC 3 #SBATCH --export=NONE #SBATCH --job-name=test_slurm #SBATCH --nodes=2 ##SBATCH --ntasks=8 #SBATCH --ntasks-per-node=3 #SBATCH --mem=10G #SBATCH --time=00:05:00 #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=tk253@le.ac.uk #SBATCH --output=%x-%j.out #SBATCH --error=%x-%j.err echo $SLURM_NTASKS echo $SLURM_NTASKS_PER_NODE echo $SLURM_JOB_NODELIST For submitting the above script to the job scheduler, use the following command. $ sbatch your_job_script.sh On DIaL3, the above script will lead to the following output: 6 3 dnode[001-002] Hint: To convert the $SLURM_JOB_NODELIST to a list of individual host names use scontrol show hostnames $SLURM_JOB_NODELIST . For example $ scontrol show hostnames dnode[001-003,005] returns: dnode001 dnode002 dnode003 dnode005 Sbatch Options --export = < [ALL,] environment variables| ALL | NONE > Identify which environment variables from the submission environment are propagated to the launched application. Note that SLURM_* variables are always propagated. --export=ALL :- Default mode if --export is not specified. All of the users environment will be loaded --export=NONE :- Only SLURM_* variables from the user environment will be defined. User must use absolute path to the binary to be executed that will define the environment. User can not specify explicit environment variables with NONE. -N, --nodes = < minnodes[-maxnodes] > Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes . If only one number is specified, this is used as both the minimum and maximum node count. -n, --ntasks = < number > sbatch does not launch tasks, it requests an allocation of resources and submits a batch script. This option advises the SLURM controller that job steps run within the allocation will launch a maximum of number tasks and to provide for sufficient resources. The default is one task per node, but note that the --cpus-per-task option will change this default. --ntasks-per-node = < ntasks > Request that ntasks be invoked on each node. If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. Meant to be used with the --nodes option. This is related to --cpus-per-task=ncpus , but does not require knowledge of the actual number of cpus on each node. --mem = < size[units] > Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. Default value is DefMemPerNode and the maximum value is MaxMemPerNode. %x Job name %j Job id of the running job. squeue It is used to view job and job step information for jobs managed by Slurm. Squeue Options -a, --all Display information about jobs and job steps in all partitions. This causes information to be displayed about partitions that are configured as hidden, partitions that are unavailable to a user's group, and federated jobs that are in a \"revoked\" state. -i < seconds >, --iterate=< seconds > Repeatedly gather and report the requested information at the interval specified (in seconds). By default, prints a time stamp with the header. -u < user_list >, --user=< user_list > Request jobs or job steps from a comma separated list of users. The list can consist of user names or user id numbers. Performance of the command can be measurably improved for systems with large numbers of jobs when a single user is specified. For example, we can use the squeue command as follows: $ squeue -a -u your_username -i 30 Summary Task Slurm Submit a job sbatch my_job_script.sh Delete a job scancel job_id Show job status squeue Show expected job start time squeue --start Show queue info sinfo Show queue details scontrol show partition Show job details scontrol show job job_id","title":"SLURM Worload Manager"},{"location":"Run_computation/Slurm_workload_manager/#slurm-workload-manager","text":"DiaL3 use SLURM as its workload manager i.e. SLURM is responsible for scheduling and running jobs on the compute nodes as and when they are available. Some of the useful SLURM commands are:- salloc :- is used to allocate resources for a job in real time. Typically this is used to allocate resources and spawn a shell. The shell is then used to execute srun commands to launch parallel tasks. sbatch :- is used to submit a job script for later execution. The script will typically contain one or more srun commands to launch parallel tasks. scancel :- is used to cancel a pending or running job or job step. It can also be used to send an arbitrary signal to all processes associated with a running job or job step. scontrol :- is the administrative tool used to view and/or modify SLURM state. Note that many of the scontrol commands can only be executed as user root. sinfo :- reports the state of partitions and nodes managed by SLURM. It has a wide variety of filtering, sorting, and formatting options. squeue :- reports the state of jobs or job steps. It has a wide variety of filtering, sorting, and formatting options. By default, it reports the running jobs in priority order and then the pending jobs in priority order. srun :- is used to submit a job for execution or initiate job steps in real time. srun command has a wide variety of options to specify resource requirements, including: minimum and maximum node count, processor count, specific nodes to use or not use, and specific node characteristics (so much memory, disk space, certain required features, etc.). A job can contain multiple job steps executing sequentially or in parallel on independent or shared resources within the job's node allocation.","title":"SLURM Workload Manager"},{"location":"Run_computation/Slurm_workload_manager/#sbatch","text":"As mnetioned above, sbatch submits a batch script to SLURM. The batch script may be given to sbatch through a file name on the command line, or if no file name is specified, sbatch will read in a script from standard input. The batch script may contain options preceded with #SBATCH before any executable commands in the script. sbatch will stop processing further #SBATCH directives once the first non-comment non-whitespace line has been reached in the script. sbatch exits immediately after the script is successfully transferred to the SLURM controller and assigned a SLURM job ID. Example: #!/bin/bash # #! Example SLURM job script for DiRAC 3 #SBATCH --export=NONE #SBATCH --job-name=test_slurm #SBATCH --nodes=2 ##SBATCH --ntasks=8 #SBATCH --ntasks-per-node=3 #SBATCH --mem=10G #SBATCH --time=00:05:00 #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=tk253@le.ac.uk #SBATCH --output=%x-%j.out #SBATCH --error=%x-%j.err echo $SLURM_NTASKS echo $SLURM_NTASKS_PER_NODE echo $SLURM_JOB_NODELIST For submitting the above script to the job scheduler, use the following command. $ sbatch your_job_script.sh On DIaL3, the above script will lead to the following output: 6 3 dnode[001-002] Hint: To convert the $SLURM_JOB_NODELIST to a list of individual host names use scontrol show hostnames $SLURM_JOB_NODELIST . For example $ scontrol show hostnames dnode[001-003,005] returns: dnode001 dnode002 dnode003 dnode005","title":"sbatch"},{"location":"Run_computation/Slurm_workload_manager/#sbatch-options","text":"--export = < [ALL,] environment variables| ALL | NONE > Identify which environment variables from the submission environment are propagated to the launched application. Note that SLURM_* variables are always propagated. --export=ALL :- Default mode if --export is not specified. All of the users environment will be loaded --export=NONE :- Only SLURM_* variables from the user environment will be defined. User must use absolute path to the binary to be executed that will define the environment. User can not specify explicit environment variables with NONE. -N, --nodes = < minnodes[-maxnodes] > Request that a minimum of minnodes nodes be allocated to this job. A maximum node count may also be specified with maxnodes . If only one number is specified, this is used as both the minimum and maximum node count. -n, --ntasks = < number > sbatch does not launch tasks, it requests an allocation of resources and submits a batch script. This option advises the SLURM controller that job steps run within the allocation will launch a maximum of number tasks and to provide for sufficient resources. The default is one task per node, but note that the --cpus-per-task option will change this default. --ntasks-per-node = < ntasks > Request that ntasks be invoked on each node. If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. Meant to be used with the --nodes option. This is related to --cpus-per-task=ncpus , but does not require knowledge of the actual number of cpus on each node. --mem = < size[units] > Specify the real memory required per node. Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. Default value is DefMemPerNode and the maximum value is MaxMemPerNode. %x Job name %j Job id of the running job.","title":"Sbatch Options"},{"location":"Run_computation/Slurm_workload_manager/#squeue","text":"It is used to view job and job step information for jobs managed by Slurm.","title":"squeue"},{"location":"Run_computation/Slurm_workload_manager/#squeue-options","text":"-a, --all Display information about jobs and job steps in all partitions. This causes information to be displayed about partitions that are configured as hidden, partitions that are unavailable to a user's group, and federated jobs that are in a \"revoked\" state. -i < seconds >, --iterate=< seconds > Repeatedly gather and report the requested information at the interval specified (in seconds). By default, prints a time stamp with the header. -u < user_list >, --user=< user_list > Request jobs or job steps from a comma separated list of users. The list can consist of user names or user id numbers. Performance of the command can be measurably improved for systems with large numbers of jobs when a single user is specified. For example, we can use the squeue command as follows: $ squeue -a -u your_username -i 30","title":"Squeue Options"},{"location":"Run_computation/Slurm_workload_manager/#summary","text":"Task Slurm Submit a job sbatch my_job_script.sh Delete a job scancel job_id Show job status squeue Show expected job start time squeue --start Show queue info sinfo Show queue details scontrol show partition Show job details scontrol show job job_id","title":"Summary"},{"location":"Run_computation/batch_system/","text":"Batch system The DiaL3 system is shared across many users and to ensure fair use everyone must do their computations by submitting jobs through a batch system that will execute the applications on the available resources. The batch system on DiAL3 is SLURM (Simple Linux Utility for Resource Management.) Creating a job script To run a job on the system you need to create a job script. A job script is a regular shell script (bash) with some directives specifying the number of CPUs, memory, etc., that will be interpreted by the batch system upon submission. You can find job script examples in Job script examples After you wrote your job script as shown in the examples, you can submit it to the scheduler by using the following command: $ sbatch jobscript.sh How to pass command-line parameters to the job script It is sometimes convenient if you do not have to edit the job script every time you want to change the input file. Or perhaps you want to submit hundreds of jobs and loop over a range of input files. For this it is handy to pass command-line parameters to the job script. In SLURM you can do this by: $ sbatch myscript.sh myinput myoutput And then you can pick the parameters up inside the job script: #!/bin/bash #SBATCH ... #SBATCH ... ... # argument 1 is myinput # argument 2 is myoutput mybinary.x < ${1} > ${2} Please note that any command starting with #SBATCH is an instruction to the scheduler and not a comment.","title":"Batch System"},{"location":"Run_computation/batch_system/#batch-system","text":"The DiaL3 system is shared across many users and to ensure fair use everyone must do their computations by submitting jobs through a batch system that will execute the applications on the available resources. The batch system on DiAL3 is SLURM (Simple Linux Utility for Resource Management.)","title":"Batch system"},{"location":"Run_computation/batch_system/#creating-a-job-script","text":"To run a job on the system you need to create a job script. A job script is a regular shell script (bash) with some directives specifying the number of CPUs, memory, etc., that will be interpreted by the batch system upon submission. You can find job script examples in Job script examples After you wrote your job script as shown in the examples, you can submit it to the scheduler by using the following command: $ sbatch jobscript.sh","title":"Creating a job script"},{"location":"Run_computation/batch_system/#how-to-pass-command-line-parameters-to-the-job-script","text":"It is sometimes convenient if you do not have to edit the job script every time you want to change the input file. Or perhaps you want to submit hundreds of jobs and loop over a range of input files. For this it is handy to pass command-line parameters to the job script. In SLURM you can do this by: $ sbatch myscript.sh myinput myoutput And then you can pick the parameters up inside the job script: #!/bin/bash #SBATCH ... #SBATCH ... ... # argument 1 is myinput # argument 2 is myoutput mybinary.x < ${1} > ${2} Please note that any command starting with #SBATCH is an instruction to the scheduler and not a comment.","title":"How to pass command-line parameters to the job script"},{"location":"Run_computation/dos_and_donts/","text":"Dos and Don'ts Please use the login nodes only for compiling and editing jobs. They should not be used for running jobs. Please be noted that jobs running on login nodes might be killed or terminated without prior notice. Always run your computations by submitting it to the queuing system i.e. SLURM on DIaL3 . Interactive jobs should also be run on the compute nodes and the same should not be run on the login nodes. You are advised to use \\scratch space for your large computations as compared to running them in your \\home as the \\home quota is quite limited.","title":"Dos and Don'ts"},{"location":"Run_computation/dos_and_donts/#dos-and-donts","text":"Please use the login nodes only for compiling and editing jobs. They should not be used for running jobs. Please be noted that jobs running on login nodes might be killed or terminated without prior notice. Always run your computations by submitting it to the queuing system i.e. SLURM on DIaL3 . Interactive jobs should also be run on the compute nodes and the same should not be run on the login nodes. You are advised to use \\scratch space for your large computations as compared to running them in your \\home as the \\home quota is quite limited.","title":"Dos and Don'ts"},{"location":"Run_computation/interactive_job/","text":"Interactive jobs Starting an interacitve job You can run an interactive job like this: $ srun --account=YOUR_ACCOUNT_NAME --nodes=NUMBER_OF_NODES_REQUIRED --ntasks-per-node=NUMBER_TASKS_PER_NODE --time=HH:MM:SS --pty bash -i An example of using the above command might look like the following on DIaL3: $ srun --nodes=2 --ntasks-per-node=8 --time=01:00:00 --pty bash -i Here we ask for a 8 cores on 2 nodes for one hour with the default amount of memory. The command prompt will appear as soon as the job starts. This is how it looks once the interactive job starts: srun: job 12345 queued and waiting for resources srun: job 12345 has been allocated resources Exit the bash shell to end the job. If you exceed the time or memory limits the job will also abort. Interactive jobs have the same policies as normal batch jobs and there are no extra restrictions. Please be advised that you might be sharing the node with other users depending on the amount of resources you have asked for.","title":"Interactive Job"},{"location":"Run_computation/interactive_job/#interactive-jobs","text":"","title":"Interactive jobs"},{"location":"Run_computation/interactive_job/#starting-an-interacitve-job","text":"You can run an interactive job like this: $ srun --account=YOUR_ACCOUNT_NAME --nodes=NUMBER_OF_NODES_REQUIRED --ntasks-per-node=NUMBER_TASKS_PER_NODE --time=HH:MM:SS --pty bash -i An example of using the above command might look like the following on DIaL3: $ srun --nodes=2 --ntasks-per-node=8 --time=01:00:00 --pty bash -i Here we ask for a 8 cores on 2 nodes for one hour with the default amount of memory. The command prompt will appear as soon as the job starts. This is how it looks once the interactive job starts: srun: job 12345 queued and waiting for resources srun: job 12345 has been allocated resources Exit the bash shell to end the job. If you exceed the time or memory limits the job will also abort. Interactive jobs have the same policies as normal batch jobs and there are no extra restrictions. Please be advised that you might be sharing the node with other users depending on the amount of resources you have asked for.","title":"Starting an interacitve job"},{"location":"Run_computation/interactive_or_batch/","text":"Interactive or batch You can run computations interactively or in batch. Interactive processing Interactive processing is the simplest way to work on a system. You log in, run commands which execute immediately, and log off when you've finished. You can use either the command line or a graphical environment. Interactive jobs run directly on the limited number of login nodes (servers) on each cluster. DiAL3 has two login nodes. Although user login sessions are spread across these nodes, interactive processes will be competing with each other. When should you use interactive processing? Short tasks Tasks that require frequent user interaction Graphically intensive tasks The design of DIaL3 is strongly biased towards batch jobs. It has 200 compute nodes and only two login nodes. Therefore interactive processes on ALICE shouldn't take more than a few minutes of CPU time. There are hard limits applied which means that a process will be killed if it uses more than a few ----- hours of CPU time on the login nodes. Furthermore, should the administrators be alerted to heavy processing on the login nodes, those processes are likely to be killed with no warning in order to ensure the login nodes remain responsive for everyone. What if you need to run longer processes interactively? You can submit jobs to the scheduler for interactive running. These jobs provide a login to one of the compute nodes with the requested resources dedicated to you. These jobs support X-forwarding so graphical applications will work as normal. For more information, see Interactive jobs . Batch processing What is batch processing? Batch processing is more complex because work has to carefully planned and able to run without user interaction. Batch jobs are submitted to a job scheduler and run on the first available compute node(s). Once submitted, you can log off and wait for the jobs to complete, with the option to be sent an email when this has happened. Although batch jobs may have to wait in a queue before running, they are then given a dedicated set of resources to ensure that they complete as quickly as possible. When should you use batch processing? Longer running processes Parallel processes Running large numbers of short jobs simultaneously Tasks that can be left running for a significant amount of time without any interaction","title":"Intercative or batch"},{"location":"Run_computation/interactive_or_batch/#interactive-or-batch","text":"You can run computations interactively or in batch.","title":"Interactive or batch"},{"location":"Run_computation/interactive_or_batch/#interactive-processing","text":"Interactive processing is the simplest way to work on a system. You log in, run commands which execute immediately, and log off when you've finished. You can use either the command line or a graphical environment. Interactive jobs run directly on the limited number of login nodes (servers) on each cluster. DiAL3 has two login nodes. Although user login sessions are spread across these nodes, interactive processes will be competing with each other.","title":"Interactive processing"},{"location":"Run_computation/interactive_or_batch/#when-should-you-use-interactive-processing","text":"Short tasks Tasks that require frequent user interaction Graphically intensive tasks The design of DIaL3 is strongly biased towards batch jobs. It has 200 compute nodes and only two login nodes. Therefore interactive processes on ALICE shouldn't take more than a few minutes of CPU time. There are hard limits applied which means that a process will be killed if it uses more than a few ----- hours of CPU time on the login nodes. Furthermore, should the administrators be alerted to heavy processing on the login nodes, those processes are likely to be killed with no warning in order to ensure the login nodes remain responsive for everyone.","title":"When should you use interactive processing?"},{"location":"Run_computation/interactive_or_batch/#what-if-you-need-to-run-longer-processes-interactively","text":"You can submit jobs to the scheduler for interactive running. These jobs provide a login to one of the compute nodes with the requested resources dedicated to you. These jobs support X-forwarding so graphical applications will work as normal. For more information, see Interactive jobs .","title":"What if you need to run longer processes interactively?"},{"location":"Run_computation/interactive_or_batch/#batch-processing","text":"","title":"Batch processing"},{"location":"Run_computation/interactive_or_batch/#what-is-batch-processing","text":"Batch processing is more complex because work has to carefully planned and able to run without user interaction. Batch jobs are submitted to a job scheduler and run on the first available compute node(s). Once submitted, you can log off and wait for the jobs to complete, with the option to be sent an email when this has happened. Although batch jobs may have to wait in a queue before running, they are then given a dedicated set of resources to ensure that they complete as quickly as possible.","title":"What is batch processing?"},{"location":"Run_computation/interactive_or_batch/#when-should-you-use-batch-processing","text":"Longer running processes Parallel processes Running large numbers of short jobs simultaneously Tasks that can be left running for a significant amount of time without any interaction","title":"When should you use batch processing?"},{"location":"Run_computation/job_types/","text":"Job types There are various types of jobs that you can run on DIaL3 such as:- Array jobs OpenMP jobs MPI jobs Hybrid MPI/OpenMP jobs We first start with a basic script explaining some details and then we present the specific example scripts of various jobs types mentioned above. Basic Job Script A basic job script on DiAL3 would look like the following. #!/bin/bash -l ############################## # BASIC JOB SCRIPT # ############################## # Define your job name here so that you can recognise it in the queue. #SBATCH --job-name=example # Define the output file name (%j at end will append the job id at the end of the name). This will store the standard output 'stdout' for the job. #SBATCH -o your_output_file_name%j # Define file name for storing any errors (stderr). #SBATCH -e your_error_file_name%j # Define the partition on which you want to run. #SBATCH -p partition_name # Define the Account/Project name from which the computation would be charged. #SBATCH -A your_Account_name # Define, how many nodes you need. Here, we ask for 1 node. Each node has 128 CPU cores. #SBATCH --nodes=1 # You can further define the number of tasks with --ntasks-per-* # See \"man sbatch\" for details. e.g. --ntasks=4 will ask for 4 cpus. # Define, how long the job will run in real time. This is a hard cap meaning # that if the job runs longer than what is written here, it will be # force-stopped by the server. If you make the expected time too long, it will # take longer for the job to start. Here, we say the job will take 1 hour. # hh:mm:ss #SBATCH --time=01:00:00 # How much memory you need. # --mem will define memory per node and # --mem-per-cpu will define memory per CPU/core. Choose one of those. #SBATCH --mem-per-cpu=1500MB ##SBATCH --mem=5GB # this one is not in effect, due to the double hash # Turn on mail notification. There are many possible self-explaining values: # NONE, BEGIN, END, FAIL, ALL (including all aforementioned) # For more values, check \"man sbatch\" #SBATCH --mail-type=END,FAIL # You may not place any commands before the last SBATCH directive # Define and create a unique scratch directory for this job SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # You can copy everything you need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/myfiles*.txt ${SCRATCH_DIRECTORY} # This is where the actual work is done. In this case, the script only waits. # The time command is optional, but it may give you a hint on how long the # command worked time sleep 10 # After the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # After everything is saved to the home directory, delete the work directory to # save space on /scratch cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script exit 0 Please note that the above script does not contain any module load command as it is a very simple and basic example. In many cases, you would need to load certain modules or compilers such as ifort , icpc , gcc , g++ etc. Please see the examples given below to see how to add module load statements in the job submission scripts. Array jobs An array job lets you submit the same job dozens or even hundreds of times with different inputs. This is a lot quicker than manually submitting the job multiple times. In this example the serial job mycode.exe will be submitted 10 times. Each job is still a serial job requesting only one processor core, but 10 instances will be queued. The sbatch option for creating an array job is --array : #SBATCH --array=1-10 Each instance of the job gets a job ID as usual, but also an array task ID. This is available within the environment variable SLURM_ARRAY_TASK_ID , and can be used within the script or program code to derive input values for the jobs. A sample submission script based on the above is given below. #!/bin/bash --login #SBATCH -J your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name #SBATCH --nodes=1 #SBATCH -t 00:30:00 # 16 jobs will run in this array at the same time #SBATCH --array=1-10 # each job will see a different ${SLURM_ARRAY_TASK_ID} echo \"now processing task id:: \" ${SLURM_ARRAY_TASK_ID} # Execute your serial job code /path_to_my_code.exe > output_${SLURM_ARRAY_TASK_ID} OpenMP jobs #!/bin/bash -l ############################# # example for an OpenMP job # ############################# #SBATCH --job-name=your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name # we ask for 1 task with 90 cores #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=90 # exclusive makes the entire node available to your job only. #SBATCH --exclusive # run for ten minutes hh:mm:ss #SBATCH --time=00:10:00 # turn on all mail notification #SBATCH --mail-type=ALL # Remove all previously loaded modules. module purge # Load your desired modules here. module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # we copy everything we need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/my_binary.x ${SCRATCH_DIRECTORY} # we set OMP_NUM_THREADS to the number of available cores export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} # we execute the job and time it time ./my_binary.x > my_output # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script and Exit. exit 0 MPI jobs #!/bin/bash -l ########################## # example for an MPI job # ########################## #SBATCH --job-name=your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name # We ask for two nodes. #SBATCH --nodes=2 # 200 MPI tasks in total #SBATCH --ntasks=200 # We ask for 100 task on each node. #SBATCH --ntasks-per-node=100 # run for one hour and thrity minutes hh:mm:ss #SBATCH --time=01:30:00 # 500MB memory per core # this is a hard limit #SBATCH --mem-per-cpu=500MB # turn on all mail notification #SBATCH --mail-type=ALL # Remove all previously loaded modules. module purge # Load your desired modules here. module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # we copy everything we need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/my_binary.x ${SCRATCH_DIRECTORY} # we execute the job and time it time mpirun -np $SLURM_NTASKS ./my_binary.x > my_output # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script and Exit. exit 0 Hybrid MPI/OpenMP jobs #!/bin/bash -l ####################################### # example for a hybrid MPI OpenMP job # ####################################### #SBATCH --job-name=your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name # we ask for 8 MPI tasks with 10 cores each #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=10 # run for fifteen minutes hh:mm:ss #SBATCH --time=00:15:00 # 500MB memory per core # this is a hard limit #SBATCH --mem-per-cpu=500MB # turn on all mail notification #SBATCH --mail-type=ALL # Remove all previously loaded modules. module purge # Load your desired modules here. module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # we copy everything we need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/my_binary.x ${SCRATCH_DIRECTORY} # we set OMP_NUM_THREADS to the number cpu cores per MPI task export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} # we execute the job and time it time mpirun -np $SLURM_NTASKS ./my_binary.x > my_output # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script and Exit. exit 0","title":"Job types"},{"location":"Run_computation/job_types/#job-types","text":"There are various types of jobs that you can run on DIaL3 such as:- Array jobs OpenMP jobs MPI jobs Hybrid MPI/OpenMP jobs We first start with a basic script explaining some details and then we present the specific example scripts of various jobs types mentioned above.","title":"Job types"},{"location":"Run_computation/job_types/#basic-job-script","text":"A basic job script on DiAL3 would look like the following. #!/bin/bash -l ############################## # BASIC JOB SCRIPT # ############################## # Define your job name here so that you can recognise it in the queue. #SBATCH --job-name=example # Define the output file name (%j at end will append the job id at the end of the name). This will store the standard output 'stdout' for the job. #SBATCH -o your_output_file_name%j # Define file name for storing any errors (stderr). #SBATCH -e your_error_file_name%j # Define the partition on which you want to run. #SBATCH -p partition_name # Define the Account/Project name from which the computation would be charged. #SBATCH -A your_Account_name # Define, how many nodes you need. Here, we ask for 1 node. Each node has 128 CPU cores. #SBATCH --nodes=1 # You can further define the number of tasks with --ntasks-per-* # See \"man sbatch\" for details. e.g. --ntasks=4 will ask for 4 cpus. # Define, how long the job will run in real time. This is a hard cap meaning # that if the job runs longer than what is written here, it will be # force-stopped by the server. If you make the expected time too long, it will # take longer for the job to start. Here, we say the job will take 1 hour. # hh:mm:ss #SBATCH --time=01:00:00 # How much memory you need. # --mem will define memory per node and # --mem-per-cpu will define memory per CPU/core. Choose one of those. #SBATCH --mem-per-cpu=1500MB ##SBATCH --mem=5GB # this one is not in effect, due to the double hash # Turn on mail notification. There are many possible self-explaining values: # NONE, BEGIN, END, FAIL, ALL (including all aforementioned) # For more values, check \"man sbatch\" #SBATCH --mail-type=END,FAIL # You may not place any commands before the last SBATCH directive # Define and create a unique scratch directory for this job SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # You can copy everything you need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/myfiles*.txt ${SCRATCH_DIRECTORY} # This is where the actual work is done. In this case, the script only waits. # The time command is optional, but it may give you a hint on how long the # command worked time sleep 10 # After the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # After everything is saved to the home directory, delete the work directory to # save space on /scratch cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script exit 0 Please note that the above script does not contain any module load command as it is a very simple and basic example. In many cases, you would need to load certain modules or compilers such as ifort , icpc , gcc , g++ etc. Please see the examples given below to see how to add module load statements in the job submission scripts.","title":"Basic Job Script"},{"location":"Run_computation/job_types/#array-jobs","text":"An array job lets you submit the same job dozens or even hundreds of times with different inputs. This is a lot quicker than manually submitting the job multiple times. In this example the serial job mycode.exe will be submitted 10 times. Each job is still a serial job requesting only one processor core, but 10 instances will be queued. The sbatch option for creating an array job is --array : #SBATCH --array=1-10 Each instance of the job gets a job ID as usual, but also an array task ID. This is available within the environment variable SLURM_ARRAY_TASK_ID , and can be used within the script or program code to derive input values for the jobs. A sample submission script based on the above is given below. #!/bin/bash --login #SBATCH -J your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name #SBATCH --nodes=1 #SBATCH -t 00:30:00 # 16 jobs will run in this array at the same time #SBATCH --array=1-10 # each job will see a different ${SLURM_ARRAY_TASK_ID} echo \"now processing task id:: \" ${SLURM_ARRAY_TASK_ID} # Execute your serial job code /path_to_my_code.exe > output_${SLURM_ARRAY_TASK_ID}","title":"Array jobs"},{"location":"Run_computation/job_types/#openmp-jobs","text":"#!/bin/bash -l ############################# # example for an OpenMP job # ############################# #SBATCH --job-name=your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name # we ask for 1 task with 90 cores #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=90 # exclusive makes the entire node available to your job only. #SBATCH --exclusive # run for ten minutes hh:mm:ss #SBATCH --time=00:10:00 # turn on all mail notification #SBATCH --mail-type=ALL # Remove all previously loaded modules. module purge # Load your desired modules here. module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # we copy everything we need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/my_binary.x ${SCRATCH_DIRECTORY} # we set OMP_NUM_THREADS to the number of available cores export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} # we execute the job and time it time ./my_binary.x > my_output # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script and Exit. exit 0","title":"OpenMP jobs"},{"location":"Run_computation/job_types/#mpi-jobs","text":"#!/bin/bash -l ########################## # example for an MPI job # ########################## #SBATCH --job-name=your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name # We ask for two nodes. #SBATCH --nodes=2 # 200 MPI tasks in total #SBATCH --ntasks=200 # We ask for 100 task on each node. #SBATCH --ntasks-per-node=100 # run for one hour and thrity minutes hh:mm:ss #SBATCH --time=01:30:00 # 500MB memory per core # this is a hard limit #SBATCH --mem-per-cpu=500MB # turn on all mail notification #SBATCH --mail-type=ALL # Remove all previously loaded modules. module purge # Load your desired modules here. module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # we copy everything we need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/my_binary.x ${SCRATCH_DIRECTORY} # we execute the job and time it time mpirun -np $SLURM_NTASKS ./my_binary.x > my_output # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script and Exit. exit 0","title":"MPI jobs"},{"location":"Run_computation/job_types/#hybrid-mpiopenmp-jobs","text":"#!/bin/bash -l ####################################### # example for a hybrid MPI OpenMP job # ####################################### #SBATCH --job-name=your_job_name #SBATCH -o output_file_name%j #SBATCH -e error_file_name%j #SBATCH -p partition_name #SBATCH -A account_name # we ask for 8 MPI tasks with 10 cores each #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=10 # run for fifteen minutes hh:mm:ss #SBATCH --time=00:15:00 # 500MB memory per core # this is a hard limit #SBATCH --mem-per-cpu=500MB # turn on all mail notification #SBATCH --mail-type=ALL # Remove all previously loaded modules. module purge # Load your desired modules here. module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 # define and create a unique scratch directory SCRATCH_DIRECTORY=/scratch/Your_project/${USER}/${SLURM_JOBID} mkdir -p ${SCRATCH_DIRECTORY} cd ${SCRATCH_DIRECTORY} # we copy everything we need to the scratch directory # ${SLURM_SUBMIT_DIR} points to the path where this script was submitted from cp ${SLURM_SUBMIT_DIR}/my_binary.x ${SCRATCH_DIRECTORY} # we set OMP_NUM_THREADS to the number cpu cores per MPI task export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK} # we execute the job and time it time mpirun -np $SLURM_NTASKS ./my_binary.x > my_output # after the job is done we copy our output back to $SLURM_SUBMIT_DIR cp ${SCRATCH_DIRECTORY}/my_output ${SLURM_SUBMIT_DIR} # we step out of the scratch directory and remove it cd ${SLURM_SUBMIT_DIR} rm -rf ${SCRATCH_DIRECTORY} # Finish the script and Exit. exit 0","title":"Hybrid MPI/OpenMP jobs"},{"location":"Run_computation/monitor_jobs/","text":"Monitor your jobs To see which of your jobs are running or to see which ones are still in queue, you can use the squeue command as follows: $ squeue -u $USER You will see an output similar to the following JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 123 parition_name my_job my_username R 00:00:15 1 node dnode001 The various fields in the above output are explained below. JOBID : Job ID assigned to your job the scheduler. PARTITION : the partition you have chosen to run your job. NAME : You job name. USER : Your username. ST : This indicates the status of the job. Some of the possible status values are: R= Running, PD = Pending and CG = Completing. TIME : Time for which the job is running in HH:MM:SS. NODES : The number of nodes you have asked for or the number of nodes that have been allocated (depending on the state of your job). NODELIST (REASON ): The nodes on which your job is running. If there is a problem with your job, it will also indicate a short message indicating the issue. To delete your job or to remove it from the queue, you can use the scancel command as shown below. $ scancel job_id","title":"Monitor jobs"},{"location":"Run_computation/monitor_jobs/#monitor-your-jobs","text":"To see which of your jobs are running or to see which ones are still in queue, you can use the squeue command as follows: $ squeue -u $USER You will see an output similar to the following JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 123 parition_name my_job my_username R 00:00:15 1 node dnode001 The various fields in the above output are explained below. JOBID : Job ID assigned to your job the scheduler. PARTITION : the partition you have chosen to run your job. NAME : You job name. USER : Your username. ST : This indicates the status of the job. Some of the possible status values are: R= Running, PD = Pending and CG = Completing. TIME : Time for which the job is running in HH:MM:SS. NODES : The number of nodes you have asked for or the number of nodes that have been allocated (depending on the state of your job). NODELIST (REASON ): The nodes on which your job is running. If there is a problem with your job, it will also indicate a short message indicating the issue. To delete your job or to remove it from the queue, you can use the scancel command as shown below. $ scancel job_id","title":"Monitor your jobs"},{"location":"Software/Python/","text":"Python Python is a high level programming language. It is an interpreted language as compared to compile type languages like C, C++ or Fortran. It is also an Object Oriented Language which allows programmers to write clear and logical code for various types of projects. It is also one of the most widely use languages in the field of Data Science, Machine Learning and Artificial Intelligence. To use Python on DIaL3, you can use one of the available modules such as the following: $ module load python3 To see the exact version of Python in your environment, use the following $ python --version It might happen that you need additional packages and libraries for Python that are not there on DIaL3. You can install them using any of the following methods:- Using Pip to Install a package in your home directory Using Virtual Environment Using Conda We describe each method in more detail below. Using Pip to Install a package in your home directory If you want to install a package in your home directory, you can use the following command $ pip install --user package_name where --user is responsible for installing the package in your home directory. Although this method is quite simple, it might lead to conflicts if you want to install multiple packages which might not be compatible with each other. The best way to handle such situation is to install separate compatible packages in their own environment. This can be done either using Virtual Environment or Using conda. Virtual environment A python virtual environment installs a python environment (and a default set of libraries) into a directory you specify. This makes it simple to use pip to install your own modules. Load the python module $ module load python3 Create the virtualenv - in this case we are creating a virtual environment in the \"my_python\" directory within the home directory. $ virtualenv --system-site-packages --prompt=$USER-my_python my_python Activate the virtual environment $ source $HOME/my_python/bin/activate On next login, you will only need to repeat the final step to load your environment and any libraries you have installed into it. To install libraries, you can simply run a standard pip command, for example, to install the gi module: $ pip install gi Conda instructions If you need a version of Python, other than the ones that are pre-installed on our HPC system, then you can install it for your account, without admin privileges, with the help of Conda. Conda is an open-source package manager and environment management system, and it is probably the most popular package manager for Python and R. There are basically two ways to install Conda; either through Anaconda, which is a distribution of Python and R for scientific computing that includes many data-science packages, or through a stripped-down version called Miniconda, which includes only Conda, Python and a small number of required packages. Since the Anaconda installation is bloated and takes up a few GBs of disk space, and a typical user will only need a small fraction of the included packages, here we will describe the installation of the latter. Miniconda installation You first need to dowload the install script from this webpage. Right-click and copy the link for the latest Linux 64bit Miniconda3 installer and then connect to DIaL3. Open a terminal and download the script with the wget command, e.g. $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -P ~ which will download it at your home directory ~ . Now navigate to your home directory with cd ~ and execute it with $ bash Miniconda3-latest-Linux-x86_64.sh Follow the instructions and press Enter to read the License Agreement, then q to go to the last page and finally type yes and press Enter if you accept the terms. You will then be asked for the directory where you want Miniconda to be installed. The default location is ~/miniconda3 and you will have to press Enter to confirm. The installer then prompts \u201c Do you wish the installer to initialize Miniconda3 by running conda init \u201c. Enter yes , then close and re-open the terminal for the changes to take effect. You will now see the word (base) just before your username, which means that the base environment is activated. You can now delete the downloaded shell script with rm Miniconda3-latest-Linux-x86_64.sh . Important : Auto-activating the base environment may cause problems accessing HPC using NX (NoMachine). We recommend that you set the auto_activate_base parameter to false, by typing: $ conda config --set auto_activate_base false If you experience problems with NoMachine then follow the instructions here . Managing conda environments To create a conda environment with a specific Python version type $ conda create -n ENVNAME python=3.8 where ENVNAME is a name of your choice. This command will install the latest Python in the 3.8 branch. To verify that the environment was successfully installed type $ conda env list which lists all your environments. If you followed the guide till here, you should see two environments; the base and the one you created in the previous step. To activate it run $ conda activate ENVNAME To install a python package on that environment, e.g. numpy type: $ conda install numpy while the environment is activated, and if you want a specific version of that package you can use $ conda install numpy=1.18.1 These commands with look for the package in the default channels. If you want to also specify a channel you can use the -c flag, e.g. $ conda install -c conda-forge numpy You can search for packages here . If a package is unavailable through conda, but is available in the Python Package Index (Pypi) repository, then you can install it with the pip command (e.g. pip install pycuda ). It is suggested that you install packages with pip only after you\u2019ve installed all the packages that you can with conda . Using conda in an HPC batch job Now, if you want to use a conda environment in a non-interactive job, you have to activate it by adding the following code in your submit script, before executing your code (change the path to the location where you\u2019ve installed miniconda): $ source ~/miniconda3/bin/activate ENVNAME Finally, to delete an environment first deactivate it with conda deactivate and then type: $ conda remove -n ENVNAME --all which will remove the environment and all installed packages.","title":"Python"},{"location":"Software/Python/#python","text":"Python is a high level programming language. It is an interpreted language as compared to compile type languages like C, C++ or Fortran. It is also an Object Oriented Language which allows programmers to write clear and logical code for various types of projects. It is also one of the most widely use languages in the field of Data Science, Machine Learning and Artificial Intelligence. To use Python on DIaL3, you can use one of the available modules such as the following: $ module load python3 To see the exact version of Python in your environment, use the following $ python --version It might happen that you need additional packages and libraries for Python that are not there on DIaL3. You can install them using any of the following methods:- Using Pip to Install a package in your home directory Using Virtual Environment Using Conda We describe each method in more detail below.","title":"Python"},{"location":"Software/Python/#using-pip-to-install-a-package-in-your-home-directory","text":"If you want to install a package in your home directory, you can use the following command $ pip install --user package_name where --user is responsible for installing the package in your home directory. Although this method is quite simple, it might lead to conflicts if you want to install multiple packages which might not be compatible with each other. The best way to handle such situation is to install separate compatible packages in their own environment. This can be done either using Virtual Environment or Using conda.","title":"Using Pip to Install a package in your home directory"},{"location":"Software/Python/#virtual-environment","text":"A python virtual environment installs a python environment (and a default set of libraries) into a directory you specify. This makes it simple to use pip to install your own modules. Load the python module $ module load python3 Create the virtualenv - in this case we are creating a virtual environment in the \"my_python\" directory within the home directory. $ virtualenv --system-site-packages --prompt=$USER-my_python my_python Activate the virtual environment $ source $HOME/my_python/bin/activate On next login, you will only need to repeat the final step to load your environment and any libraries you have installed into it. To install libraries, you can simply run a standard pip command, for example, to install the gi module: $ pip install gi","title":"Virtual environment"},{"location":"Software/Python/#conda-instructions","text":"If you need a version of Python, other than the ones that are pre-installed on our HPC system, then you can install it for your account, without admin privileges, with the help of Conda. Conda is an open-source package manager and environment management system, and it is probably the most popular package manager for Python and R. There are basically two ways to install Conda; either through Anaconda, which is a distribution of Python and R for scientific computing that includes many data-science packages, or through a stripped-down version called Miniconda, which includes only Conda, Python and a small number of required packages. Since the Anaconda installation is bloated and takes up a few GBs of disk space, and a typical user will only need a small fraction of the included packages, here we will describe the installation of the latter.","title":"Conda instructions"},{"location":"Software/Python/#miniconda-installation","text":"You first need to dowload the install script from this webpage. Right-click and copy the link for the latest Linux 64bit Miniconda3 installer and then connect to DIaL3. Open a terminal and download the script with the wget command, e.g. $ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -P ~ which will download it at your home directory ~ . Now navigate to your home directory with cd ~ and execute it with $ bash Miniconda3-latest-Linux-x86_64.sh Follow the instructions and press Enter to read the License Agreement, then q to go to the last page and finally type yes and press Enter if you accept the terms. You will then be asked for the directory where you want Miniconda to be installed. The default location is ~/miniconda3 and you will have to press Enter to confirm. The installer then prompts \u201c Do you wish the installer to initialize Miniconda3 by running conda init \u201c. Enter yes , then close and re-open the terminal for the changes to take effect. You will now see the word (base) just before your username, which means that the base environment is activated. You can now delete the downloaded shell script with rm Miniconda3-latest-Linux-x86_64.sh . Important : Auto-activating the base environment may cause problems accessing HPC using NX (NoMachine). We recommend that you set the auto_activate_base parameter to false, by typing: $ conda config --set auto_activate_base false If you experience problems with NoMachine then follow the instructions here .","title":"Miniconda installation"},{"location":"Software/Python/#managing-conda-environments","text":"To create a conda environment with a specific Python version type $ conda create -n ENVNAME python=3.8 where ENVNAME is a name of your choice. This command will install the latest Python in the 3.8 branch. To verify that the environment was successfully installed type $ conda env list which lists all your environments. If you followed the guide till here, you should see two environments; the base and the one you created in the previous step. To activate it run $ conda activate ENVNAME To install a python package on that environment, e.g. numpy type: $ conda install numpy while the environment is activated, and if you want a specific version of that package you can use $ conda install numpy=1.18.1 These commands with look for the package in the default channels. If you want to also specify a channel you can use the -c flag, e.g. $ conda install -c conda-forge numpy You can search for packages here . If a package is unavailable through conda, but is available in the Python Package Index (Pypi) repository, then you can install it with the pip command (e.g. pip install pycuda ). It is suggested that you install packages with pip only after you\u2019ve installed all the packages that you can with conda .","title":"Managing conda environments"},{"location":"Software/Python/#using-conda-in-an-hpc-batch-job","text":"Now, if you want to use a conda environment in a non-interactive job, you have to activate it by adding the following code in your submit script, before executing your code (change the path to the location where you\u2019ve installed miniconda): $ source ~/miniconda3/bin/activate ENVNAME Finally, to delete an environment first deactivate it with conda deactivate and then type: $ conda remove -n ENVNAME --all which will remove the environment and all installed packages.","title":"Using conda in an HPC batch job"},{"location":"Software/module_system/","text":"Software Module System Since DIaL3 is a shared resource used by many researchers, students and academicians, the Research Computing Services (RCS) team provides some centrally installed software for all users. This allows two major advantages:- It allows for easier support and maintenance of the software as the user cannot change the default settings in a centrally installed software. A user is relieved of installing a software by himself as many of the most widely used software have already been installed for them and are ready to be used. The main command for using this system is called the module command. To get a list of all options available with this command, you can type: $ module --help Which all modules are available? To see a list of all modules installed on DIaL3, use the command: $ module avail For more details, please see List of all modules Which modules are currently loaded? To see the list of module currently loaded in your environment, please use the command: $ module list How to load a module? To load a module, type the following in you terminal: $ module load gcc This will load the default gcc version ( gcc/10.2.0 in this case ) in your environment. To load a particular version, please use: $ module load gcc/11.1.0/xu5zmz How to unload a module? To unload a module, please use the following: $ module unload gcc/11.1.0/xu5zmz How to see which all environment variables are modified by loading a module? To get more details about all the environmental variables that are added or modified by loading a module, please use the either of the two commands module display gcc or module show gcc . For example, if you type the following in your terminal, $ module display gcc You will get an output similar to the following: ---------------------------------------------------------------------------- /cm/local/modulefiles/gcc/10.2.0: ---------------------------------------------------------------------------- whatis(\"adds GNU Cross Compilers to your environment variables \") prepend_path(\"PATH\",\"/cm/local/apps/gcc/10.2.0/bin\") prepend_path(\"LD_LIBRARY_PATH\",\"/cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32\") help([[ Adds GNU Cross Compilers to your environment variables, ]]) From above output, it can be clearly seen that the gcc module adds values to the PATH and LD_LIBRARY_PATH variables. How to switch to a different version of a module? To switch to a different version of a module, you can use the module switch older_version new_version command as shown below. $ module switch gcc gcc/11.1.0 How to unload all modules? To remove all modules from your environment, please use the following command. $ module purge","title":"Software Module System"},{"location":"Software/module_system/#software-module-system","text":"Since DIaL3 is a shared resource used by many researchers, students and academicians, the Research Computing Services (RCS) team provides some centrally installed software for all users. This allows two major advantages:- It allows for easier support and maintenance of the software as the user cannot change the default settings in a centrally installed software. A user is relieved of installing a software by himself as many of the most widely used software have already been installed for them and are ready to be used. The main command for using this system is called the module command. To get a list of all options available with this command, you can type: $ module --help","title":"Software Module System"},{"location":"Software/module_system/#which-all-modules-are-available","text":"To see a list of all modules installed on DIaL3, use the command: $ module avail For more details, please see List of all modules","title":"Which all modules are available?"},{"location":"Software/module_system/#which-modules-are-currently-loaded","text":"To see the list of module currently loaded in your environment, please use the command: $ module list","title":"Which modules are currently loaded?"},{"location":"Software/module_system/#how-to-load-a-module","text":"To load a module, type the following in you terminal: $ module load gcc This will load the default gcc version ( gcc/10.2.0 in this case ) in your environment. To load a particular version, please use: $ module load gcc/11.1.0/xu5zmz","title":"How to load a module?"},{"location":"Software/module_system/#how-to-unload-a-module","text":"To unload a module, please use the following: $ module unload gcc/11.1.0/xu5zmz","title":"How to unload a module?"},{"location":"Software/module_system/#how-to-see-which-all-environment-variables-are-modified-by-loading-a-module","text":"To get more details about all the environmental variables that are added or modified by loading a module, please use the either of the two commands module display gcc or module show gcc . For example, if you type the following in your terminal, $ module display gcc You will get an output similar to the following: ---------------------------------------------------------------------------- /cm/local/modulefiles/gcc/10.2.0: ---------------------------------------------------------------------------- whatis(\"adds GNU Cross Compilers to your environment variables \") prepend_path(\"PATH\",\"/cm/local/apps/gcc/10.2.0/bin\") prepend_path(\"LD_LIBRARY_PATH\",\"/cm/local/apps/gcc/10.2.0/lib:/cm/local/apps/gcc/10.2.0/lib64:/cm/local/apps/gcc/10.2.0/lib32\") help([[ Adds GNU Cross Compilers to your environment variables, ]]) From above output, it can be clearly seen that the gcc module adds values to the PATH and LD_LIBRARY_PATH variables.","title":"How to see which all environment variables are modified by loading a module?"},{"location":"Software/module_system/#how-to-switch-to-a-different-version-of-a-module","text":"To switch to a different version of a module, you can use the module switch older_version new_version command as shown below. $ module switch gcc gcc/11.1.0","title":"How to switch to a different version of a module?"},{"location":"Software/module_system/#how-to-unload-all-modules","text":"To remove all modules from your environment, please use the following command. $ module purge","title":"How to unload all modules?"},{"location":"Software/software_installed/","text":"Software Installed on DIaL3 To view the list of all software installed on DIaL3, please use the following command. $ module avail This will produce an output similar to the one given below. ----------------------------------------------------------------------------------------------------------- /cm/local/modulefiles ------------------------------------------------------------------------------------------------------------ boost/1.74.0 cm-bios-tools cmjob freeipmi/1.6.6 ipmitool/1.8.18 luajit module-git null python3 shared cluster-tools/9.1 cmd dot gcc/10.2.0 (L) lua/5.4.0 mariadb-libs module-info openldap python37 ----------------------------------------------------------------------------------------------------------- /cm/shared/modulefiles ----------------------------------------------------------------------------------------------------------- arm/forge/21.0.2 cm-pmix3/3.1.4 globalarrays/openmpi/gcc/64/5.7 hwloc/1.11.11 (L) lapack/gcc/64/3.9.0 netperf/2.7.0 openmpi/intel/64/1.10.7 blacs/openmpi/gcc/64/1.1patch03 default-environment hdf5/1.10.1 intel-cluster-runtime/ia32/2018.4 mpich/ge/gcc/64/3.3.2 openblas/dynamic/0.3.7 openmpi4/intel/4.0.5 blas/gcc/64/3.8.0 fftw3/openmpi/gcc/64/3.3.8 hdf5_18/1.8.21 intel-cluster-runtime/intel64/2018.4 (D) mvapich2/gcc/64/2.3.4 openmpi/gcc/64/1.10.7 ucx/1.8.1 bonnie++/1.98 gdb/9.2 hpl/2.3 iozone/3_490 netcdf/gcc/64/gcc/64/4.7.3 openmpi/intel/4.1.1 ------------------------------------------------------------------------------------------- /cm/shared/spack/modulefiles/linux-centos8-x86_64/Core ------------------------------------------------------------------------------------------- aocc/3.0.0/khmpxw git/2.31.1/5gqme4 (D) libyaml/0.2.5/munqv6 py-astropy/4.0.1.post1/y7idi7 py-pip/20.2/dpkoc3 py-six/1.15.0/qdn5ig subversion/1.14.0/phet6w cfitsio/3.49/bz5el3 gmp/6.2.1/duarub mpc/1.1.0/gq23h3 py-attrs/20.3.0/gdqkh2 py-pybind11/2.5.0/2u5jvb py-urllib3/1.25.6/jteu6y utf8proc/2.4.0/z2q6eq erfa/1.7.0/tglix3 go/1.16.3/vluwso mpc/1.1.0/qjhl6d (D) py-certifi/2020.6.20/3sawz2 py-pyrsistent/0.15.7/2rz44t python/3.8.10/k3bk3z wcslib/7.3/qlpvcj gcc/10.1.0/jc4hra intel-oneapi-compilers/2021.2.0/3q6eev mpfr/3.1.6/gp2hzx py-chardet/3.0.4/224gjx py-pyyaml/5.3.1/dabygq reframe/3.6.0/uybl7o zstd/1.5.0/smjvvt gcc/10.3.0/picedk intel-parallel-studio/cluster.2019.5/aveeft mpfr/4.1.0/5s7ta5 py-idna/2.8/zbo4e6 py-requests/2.24.0/g5snxt shadow/4.8.1/uh7fcd gcc/11.1.0/xu5zmz json-c/0.15/gl7632 openblas/0.3.15/lgxqon py-jsonschema/3.2.0/gch7xg py-scipy/1.6.3/6oolhg singularity/3.7.2/s3zh3a gcc/9.3.0/jxhhag libgcrypt/1.9.1/mqhgzp popt/1.16/jbnhdc py-lxml/4.6.1/5dwx7l py-semver/2.8.1/hvljd6 slurm/20.11/4tobjv git/2.31.1/uqsg2n libxslt/1.1.33/67rmne py-argcomplete/1.12.0/bdomkm py-numpy/1.20.3/koxiyg py-setuptools/50.3.2/huv7bg squashfs/4.4/h6onej Where: L: Module is loaded D: Default Module Module defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree. See https://lmod.readthedocs.io/en/latest/060_locating.html for details. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". Search for a module There are two ways by which you can search for a particular module of your interest. Using module avail directly Using spider command Using module avail to search a module Let us say that you want to search for all modules having intel word in their name. You can use the following command in your terminal. $ module avail intel This will give you the following output. ---------------------------- /cm/shared/modulefiles ---------------------------- intel-cluster-runtime/ia32/2018.4 openmpi/intel/64/1.10.7 intel-cluster-runtime/intel64/2018.4 (D) openmpi4/intel/4.0.5 openmpi/intel/4.1.1 ------------ /cm/shared/spack/modulefiles/linux-centos8-x86_64/Core ------------ intel-oneapi-compilers/2021.2.0/3q6eev intel-parallel-studio/cluster.2019.5/aveeft Where: D: Default Module Module defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree. See https://lmod.readthedocs.io/en/latest/060_locating.html for details. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". As you can see in the output, you got all modules with the name intel in them but it also produced some extra lines at the end. In order to further refine the output, you can use grep command in conjunction with module avail as shown below. $ module avail 2>&1 | grep -i intel The above command will produce the output as shown below. intel-cluster-runtime/ia32/2018.4 intel-cluster-runtime/intel64/2018.4 (D) openmpi/intel/4.1.1 openmpi/intel/64/1.10.7 openmpi4/intel/4.0.5 intel-oneapi-compilers/2021.2.0/3q6eev intel-parallel-studio/cluster.2019.5/aveeft Using spider to search a module You can also use spider to search a module as shown below. $ module spider intel This will show an output similar to what is shown below. ---------------------------------------------------------------------------- intel-cluster-runtime: ---------------------------------------------------------------------------- Versions: intel-cluster-runtime/ia32/2018.4 intel-cluster-runtime/intel64/2018.4 ---------------------------------------------------------------------------- For detailed information about a specific \"intel-cluster-runtime\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider intel-cluster-runtime/intel64/2018.4 ---------------------------------------------------------------------------- ---------------------------------------------------------------------------- intel-oneapi-compilers/2021.2.0: intel-oneapi-compilers/2021.2.0/3q6eev ---------------------------------------------------------------------------- This module can be loaded directly: module load intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel OneAPI compilers Provides Classic and Beta compilers for: Fortran, C, C++ ---------------------------------------------------------------------------- intel-oneapi-mkl/2021.2.0: intel-oneapi-mkl/2021.2.0/gbymet ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"intel-oneapi-mkl/2021.2.0/gbymet\" module is available to load. intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel oneAPI MKL. ---------------------------------------------------------------------------- intel-oneapi-mpi/2021.2.0: intel-oneapi-mpi/2021.2.0/jjwa2o ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"intel-oneapi-mpi/2021.2.0/jjwa2o\" module is available to load. intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel oneAPI MPI. ---------------------------------------------------------------------------- intel-oneapi-tbb/2021.2.0: intel-oneapi-tbb/2021.2.0/lv2dqa ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"intel-oneapi-tbb/2021.2.0/lv2dqa\" module is available to load. intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel oneAPI TBB. ---------------------------------------------------------------------------- intel-parallel-studio/cluster.2019.5: intel-parallel-studio/cluster.2019.5/aveeft ---------------------------------------------------------------------------- This module can be loaded directly: module load intel-parallel-studio/cluster.2019.5/aveeft Help: Intel Parallel Studio. ---------------------------------------------------------------------------- openmpi/intel/4.1.1: openmpi/intel/4.1.1 ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"openmpi/intel/4.1.1\" module is available to load. shared Help: Adds OpenMPI to your environment variables, ---------------------------------------------------------------------------- openmpi/intel/64: openmpi/intel/64/1.10.7 ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"openmpi/intel/64/1.10.7\" module is available to load. shared Help: Adds OpenMPI to your environment variables, ---------------------------------------------------------------------------- openmpi4/intel: openmpi4/intel/4.0.5 ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"openmpi4/intel/4.0.5\" module is available to load. shared Help: Adds OpenMPI 4 to your environment variables,","title":"Software Installed on DIaL3"},{"location":"Software/software_installed/#software-installed-on-dial3","text":"To view the list of all software installed on DIaL3, please use the following command. $ module avail This will produce an output similar to the one given below. ----------------------------------------------------------------------------------------------------------- /cm/local/modulefiles ------------------------------------------------------------------------------------------------------------ boost/1.74.0 cm-bios-tools cmjob freeipmi/1.6.6 ipmitool/1.8.18 luajit module-git null python3 shared cluster-tools/9.1 cmd dot gcc/10.2.0 (L) lua/5.4.0 mariadb-libs module-info openldap python37 ----------------------------------------------------------------------------------------------------------- /cm/shared/modulefiles ----------------------------------------------------------------------------------------------------------- arm/forge/21.0.2 cm-pmix3/3.1.4 globalarrays/openmpi/gcc/64/5.7 hwloc/1.11.11 (L) lapack/gcc/64/3.9.0 netperf/2.7.0 openmpi/intel/64/1.10.7 blacs/openmpi/gcc/64/1.1patch03 default-environment hdf5/1.10.1 intel-cluster-runtime/ia32/2018.4 mpich/ge/gcc/64/3.3.2 openblas/dynamic/0.3.7 openmpi4/intel/4.0.5 blas/gcc/64/3.8.0 fftw3/openmpi/gcc/64/3.3.8 hdf5_18/1.8.21 intel-cluster-runtime/intel64/2018.4 (D) mvapich2/gcc/64/2.3.4 openmpi/gcc/64/1.10.7 ucx/1.8.1 bonnie++/1.98 gdb/9.2 hpl/2.3 iozone/3_490 netcdf/gcc/64/gcc/64/4.7.3 openmpi/intel/4.1.1 ------------------------------------------------------------------------------------------- /cm/shared/spack/modulefiles/linux-centos8-x86_64/Core ------------------------------------------------------------------------------------------- aocc/3.0.0/khmpxw git/2.31.1/5gqme4 (D) libyaml/0.2.5/munqv6 py-astropy/4.0.1.post1/y7idi7 py-pip/20.2/dpkoc3 py-six/1.15.0/qdn5ig subversion/1.14.0/phet6w cfitsio/3.49/bz5el3 gmp/6.2.1/duarub mpc/1.1.0/gq23h3 py-attrs/20.3.0/gdqkh2 py-pybind11/2.5.0/2u5jvb py-urllib3/1.25.6/jteu6y utf8proc/2.4.0/z2q6eq erfa/1.7.0/tglix3 go/1.16.3/vluwso mpc/1.1.0/qjhl6d (D) py-certifi/2020.6.20/3sawz2 py-pyrsistent/0.15.7/2rz44t python/3.8.10/k3bk3z wcslib/7.3/qlpvcj gcc/10.1.0/jc4hra intel-oneapi-compilers/2021.2.0/3q6eev mpfr/3.1.6/gp2hzx py-chardet/3.0.4/224gjx py-pyyaml/5.3.1/dabygq reframe/3.6.0/uybl7o zstd/1.5.0/smjvvt gcc/10.3.0/picedk intel-parallel-studio/cluster.2019.5/aveeft mpfr/4.1.0/5s7ta5 py-idna/2.8/zbo4e6 py-requests/2.24.0/g5snxt shadow/4.8.1/uh7fcd gcc/11.1.0/xu5zmz json-c/0.15/gl7632 openblas/0.3.15/lgxqon py-jsonschema/3.2.0/gch7xg py-scipy/1.6.3/6oolhg singularity/3.7.2/s3zh3a gcc/9.3.0/jxhhag libgcrypt/1.9.1/mqhgzp popt/1.16/jbnhdc py-lxml/4.6.1/5dwx7l py-semver/2.8.1/hvljd6 slurm/20.11/4tobjv git/2.31.1/uqsg2n libxslt/1.1.33/67rmne py-argcomplete/1.12.0/bdomkm py-numpy/1.20.3/koxiyg py-setuptools/50.3.2/huv7bg squashfs/4.4/h6onej Where: L: Module is loaded D: Default Module Module defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree. See https://lmod.readthedocs.io/en/latest/060_locating.html for details. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".","title":"Software Installed on DIaL3"},{"location":"Software/software_installed/#search-for-a-module","text":"There are two ways by which you can search for a particular module of your interest. Using module avail directly Using spider command","title":"Search for a module"},{"location":"Software/software_installed/#using-module-avail-to-search-a-module","text":"Let us say that you want to search for all modules having intel word in their name. You can use the following command in your terminal. $ module avail intel This will give you the following output. ---------------------------- /cm/shared/modulefiles ---------------------------- intel-cluster-runtime/ia32/2018.4 openmpi/intel/64/1.10.7 intel-cluster-runtime/intel64/2018.4 (D) openmpi4/intel/4.0.5 openmpi/intel/4.1.1 ------------ /cm/shared/spack/modulefiles/linux-centos8-x86_64/Core ------------ intel-oneapi-compilers/2021.2.0/3q6eev intel-parallel-studio/cluster.2019.5/aveeft Where: D: Default Module Module defaults are chosen based on Find First Rules due to Name/Version/Version modules found in the module tree. See https://lmod.readthedocs.io/en/latest/060_locating.html for details. Use \"module spider\" to find all possible modules and extensions. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". As you can see in the output, you got all modules with the name intel in them but it also produced some extra lines at the end. In order to further refine the output, you can use grep command in conjunction with module avail as shown below. $ module avail 2>&1 | grep -i intel The above command will produce the output as shown below. intel-cluster-runtime/ia32/2018.4 intel-cluster-runtime/intel64/2018.4 (D) openmpi/intel/4.1.1 openmpi/intel/64/1.10.7 openmpi4/intel/4.0.5 intel-oneapi-compilers/2021.2.0/3q6eev intel-parallel-studio/cluster.2019.5/aveeft","title":"Using module avail to search a module"},{"location":"Software/software_installed/#using-spider-to-search-a-module","text":"You can also use spider to search a module as shown below. $ module spider intel This will show an output similar to what is shown below. ---------------------------------------------------------------------------- intel-cluster-runtime: ---------------------------------------------------------------------------- Versions: intel-cluster-runtime/ia32/2018.4 intel-cluster-runtime/intel64/2018.4 ---------------------------------------------------------------------------- For detailed information about a specific \"intel-cluster-runtime\" package (including how to load the modules) use the module's full name. Note that names that have a trailing (E) are extensions provided by other modules. For example: $ module spider intel-cluster-runtime/intel64/2018.4 ---------------------------------------------------------------------------- ---------------------------------------------------------------------------- intel-oneapi-compilers/2021.2.0: intel-oneapi-compilers/2021.2.0/3q6eev ---------------------------------------------------------------------------- This module can be loaded directly: module load intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel OneAPI compilers Provides Classic and Beta compilers for: Fortran, C, C++ ---------------------------------------------------------------------------- intel-oneapi-mkl/2021.2.0: intel-oneapi-mkl/2021.2.0/gbymet ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"intel-oneapi-mkl/2021.2.0/gbymet\" module is available to load. intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel oneAPI MKL. ---------------------------------------------------------------------------- intel-oneapi-mpi/2021.2.0: intel-oneapi-mpi/2021.2.0/jjwa2o ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"intel-oneapi-mpi/2021.2.0/jjwa2o\" module is available to load. intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel oneAPI MPI. ---------------------------------------------------------------------------- intel-oneapi-tbb/2021.2.0: intel-oneapi-tbb/2021.2.0/lv2dqa ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"intel-oneapi-tbb/2021.2.0/lv2dqa\" module is available to load. intel-oneapi-compilers/2021.2.0/3q6eev Help: Intel oneAPI TBB. ---------------------------------------------------------------------------- intel-parallel-studio/cluster.2019.5: intel-parallel-studio/cluster.2019.5/aveeft ---------------------------------------------------------------------------- This module can be loaded directly: module load intel-parallel-studio/cluster.2019.5/aveeft Help: Intel Parallel Studio. ---------------------------------------------------------------------------- openmpi/intel/4.1.1: openmpi/intel/4.1.1 ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"openmpi/intel/4.1.1\" module is available to load. shared Help: Adds OpenMPI to your environment variables, ---------------------------------------------------------------------------- openmpi/intel/64: openmpi/intel/64/1.10.7 ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"openmpi/intel/64/1.10.7\" module is available to load. shared Help: Adds OpenMPI to your environment variables, ---------------------------------------------------------------------------- openmpi4/intel: openmpi4/intel/4.0.5 ---------------------------------------------------------------------------- You will need to load all module(s) on any one of the lines below before the \"openmpi4/intel/4.0.5\" module is available to load. shared Help: Adds OpenMPI 4 to your environment variables,","title":"Using spider to search a module"},{"location":"Support/Contact/","text":"Contact If you need any assitance in using DIaL3 services, please send a suport request to leicester.support@dirac.ac.uk , and our team will try to help you as soon as possible. In your Email, please share as much detail as possible about your query. Image Credit: https://xkcd.com/293/","title":"Contact"},{"location":"Support/Contact/#contact","text":"If you need any assitance in using DIaL3 services, please send a suport request to leicester.support@dirac.ac.uk , and our team will try to help you as soon as possible. In your Email, please share as much detail as possible about your query. Image Credit: https://xkcd.com/293/","title":"Contact"},{"location":"Support/FAQ/","text":"How do I check my disk usage / quota? How do I check my allocation?","title":"Frequently asked questions"},{"location":"Support/FAQ/#how-do-i-check-my-disk-usage-quota","text":"","title":"How do I check my disk usage / quota?"},{"location":"Support/FAQ/#how-do-i-check-my-allocation","text":"","title":"How do I check my allocation?"},{"location":"Support/Support_staff/","text":"Meet our Team The support team for DIaL3 consists of the following members. JonWakelin :- Team lead at ITS Liam Gretton :- Gary Gilchrist :- Christopher Mountford :- Makis Kappas :- Research Software Engineer Lokesh Ragta :- Research Software Engineer","title":"Support Staff"},{"location":"Support/Support_staff/#meet-our-team","text":"The support team for DIaL3 consists of the following members. JonWakelin :- Team lead at ITS Liam Gretton :- Gary Gilchrist :- Christopher Mountford :- Makis Kappas :- Research Software Engineer Lokesh Ragta :- Research Software Engineer","title":"Meet our Team"}]}