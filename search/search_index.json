{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DIaL3 The University of Leicester hosts one of the four High Performance Computing (HPC) systems that forms the part of Distributed Research Utilising Advanced Computing (DiRAC). The other three sites are: Cambridge, Durham and Edinburgh. The DiRAC HPC resources are classified into 4 categories namely:- Data Intensive at Leicester (DIaL) Data Intensive at Cambridge Memory Intensive at Durham Extreme Scaling at Edinburgh For more information about DiRAC service, please see the link: https://dirac.ac.uk/ . Data Intensive at Leicester (DIaL) is a term collectively used for two HPC systems at UoL: DiAL2.5 and DiAL3. For more information about DiAL2.5, please follow Dial2.5 (You will have to login using your UoL username and password). DiAL3 is a new HPC facility that has been recently purchased under the grant of 2 million pounds provided by Science and Technology Facilities Council (STFC). The system will be available for production usage from September 30, 2021. The following image shows the DiAL3 HPC facility at UoL. For more information about DiAL3 and its architecture, please visit the following links. Architecture FileSystem","title":"Home"},{"location":"#dial3","text":"The University of Leicester hosts one of the four High Performance Computing (HPC) systems that forms the part of Distributed Research Utilising Advanced Computing (DiRAC). The other three sites are: Cambridge, Durham and Edinburgh. The DiRAC HPC resources are classified into 4 categories namely:- Data Intensive at Leicester (DIaL) Data Intensive at Cambridge Memory Intensive at Durham Extreme Scaling at Edinburgh For more information about DiRAC service, please see the link: https://dirac.ac.uk/ . Data Intensive at Leicester (DIaL) is a term collectively used for two HPC systems at UoL: DiAL2.5 and DiAL3. For more information about DiAL2.5, please follow Dial2.5 (You will have to login using your UoL username and password). DiAL3 is a new HPC facility that has been recently purchased under the grant of 2 million pounds provided by Science and Technology Facilities Council (STFC). The system will be available for production usage from September 30, 2021. The following image shows the DiAL3 HPC facility at UoL. For more information about DiAL3 and its architecture, please visit the following links. Architecture FileSystem","title":"DIaL3"},{"location":"About_dial3/architecture/","text":"DIaL3 Technical Specifications The DIaL3 system has a total of 25,600 AMD EPYC compute cores whic is equivalent to 57.6 Tera flops of computing power. The detailed specifications are as follows:- Compute nodes: The system comprise of 200 compute nodes each having 2 x AMD EPYC 7742 (Rome) processors. Each processor has 64 cores running at a base frequency of 2.25 GHz thereby leading to 128 cores per nodes and a total of 25,600 cores in total. Login nodes : There are two login nodes comprising of 2 x AMD EPYC 7502 processors. Each processor has 32-Cores thereby leading to a total of 128 cores on login nodes. RAM on each compute node: Each compute node consists of 512 GB of DDR4 RAM thereby providing 4 GB of RAM for each core. Interconnect: Infiniband ------ Storage: 3PB ----- Operating System: Centos (Version ---) Job Scheduler: Slurm Workload manager","title":"Architecture"},{"location":"About_dial3/architecture/#dial3-technical-specifications","text":"The DIaL3 system has a total of 25,600 AMD EPYC compute cores whic is equivalent to 57.6 Tera flops of computing power. The detailed specifications are as follows:- Compute nodes: The system comprise of 200 compute nodes each having 2 x AMD EPYC 7742 (Rome) processors. Each processor has 64 cores running at a base frequency of 2.25 GHz thereby leading to 128 cores per nodes and a total of 25,600 cores in total. Login nodes : There are two login nodes comprising of 2 x AMD EPYC 7502 processors. Each processor has 32-Cores thereby leading to a total of 128 cores on login nodes. RAM on each compute node: Each compute node consists of 512 GB of DDR4 RAM thereby providing 4 GB of RAM for each core. Interconnect: Infiniband ------ Storage: 3PB ----- Operating System: Centos (Version ---) Job Scheduler: Slurm Workload manager","title":"DIaL3 Technical Specifications"},{"location":"About_dial3/filesystem/","text":"File Systems on DIaL3 There are a number of file systems on DIaL3 that should be used in different ways: /home /scratch /data /tmp /home Every user has a home directory within the /home file system. This will be your current working directory when you login to DIaL3. The home directory should only be used to store program code, job submission scripts, configuration files and small amounts of data. Your home directory can be referenced on the Linux command line with the shorthand ~ . The simple command cd with no arguments will return you to your home directory. Users' home directories are backed up nightly. /scratch You will have a scratch directory on the scratch file system for each project that you are associated with. The location will be /scratch/project/username - so users in multiple projects will have several to choose from. As each user's scratch directory is readable by other members of the same project, it's important to choose the correct one depending on which project is being worked on. The directory /scratch/project/shared has special permissions to ensure that all files within are always owned by the project group. The scratch directory used should be the main location for job files, and generally should be used as the working directory for jobs. The scratch space has quotas applied which are in line with those requested for \u201cwork\u201d in DiRAC time applications. These are not allocations, only quotas, so can add up to more than the available storage. Warning: Files within /scratch are not backed up . Furthermore, there is an automated process which deletes any files that haven't been accessed in more than 60 days . There will be no prior warning that files will be deleted, it is up to users to ensure that important data is not kept in /scratch for long term storage. /data This file system is provided for medium term storage of results prior to publication and transfer of data to the users own institution. The structure of the filesystem is similar to /scratch . It is also not backed up and will be swept of old data. The data sweeping will be based on modification times being more than 9 months. /tmp Each compute node has a small amount of local disc mounted on /tmp . For some jobs there may be a performance gain over /tmp in using this file system for intermediate files. Standard compute nodes have 100GB available. The preferred way to use /tmp within a job is to refer to it using the environment variable TMPDIR . This way the job's files are cleaned-up when the job finishes automatically. Otherwise it is your job's responsibility to remove files from the local file system when it ends. Warning: The /tmp file system is not backed up, and the contents are deleted whenever the compute node reboots for any reason. Files on /tmp should only be considered safe for the duration of the job which they belong to.","title":"Filesystem"},{"location":"About_dial3/filesystem/#file-systems-on-dial3","text":"There are a number of file systems on DIaL3 that should be used in different ways: /home /scratch /data /tmp","title":"File Systems on DIaL3"},{"location":"About_dial3/filesystem/#home","text":"Every user has a home directory within the /home file system. This will be your current working directory when you login to DIaL3. The home directory should only be used to store program code, job submission scripts, configuration files and small amounts of data. Your home directory can be referenced on the Linux command line with the shorthand ~ . The simple command cd with no arguments will return you to your home directory. Users' home directories are backed up nightly.","title":"/home"},{"location":"About_dial3/filesystem/#scratch","text":"You will have a scratch directory on the scratch file system for each project that you are associated with. The location will be /scratch/project/username - so users in multiple projects will have several to choose from. As each user's scratch directory is readable by other members of the same project, it's important to choose the correct one depending on which project is being worked on. The directory /scratch/project/shared has special permissions to ensure that all files within are always owned by the project group. The scratch directory used should be the main location for job files, and generally should be used as the working directory for jobs. The scratch space has quotas applied which are in line with those requested for \u201cwork\u201d in DiRAC time applications. These are not allocations, only quotas, so can add up to more than the available storage. Warning: Files within /scratch are not backed up . Furthermore, there is an automated process which deletes any files that haven't been accessed in more than 60 days . There will be no prior warning that files will be deleted, it is up to users to ensure that important data is not kept in /scratch for long term storage.","title":"/scratch"},{"location":"About_dial3/filesystem/#data","text":"This file system is provided for medium term storage of results prior to publication and transfer of data to the users own institution. The structure of the filesystem is similar to /scratch . It is also not backed up and will be swept of old data. The data sweeping will be based on modification times being more than 9 months.","title":"/data"},{"location":"About_dial3/filesystem/#tmp","text":"Each compute node has a small amount of local disc mounted on /tmp . For some jobs there may be a performance gain over /tmp in using this file system for intermediate files. Standard compute nodes have 100GB available. The preferred way to use /tmp within a job is to refer to it using the environment variable TMPDIR . This way the job's files are cleaned-up when the job finishes automatically. Otherwise it is your job's responsibility to remove files from the local file system when it ends. Warning: The /tmp file system is not backed up, and the contents are deleted whenever the compute node reboots for any reason. Files on /tmp should only be considered safe for the duration of the job which they belong to.","title":"/tmp"},{"location":"Analyse_code/Debug_code/","text":"Under Construction","title":"Debug your Code"},{"location":"Analyse_code/Install_arm_forge/","text":"Install Arm forge on your PC You can install Arm forge on your local system and use the locally installed Arm forge to view your code profile. This method will be faster that X forwarding (as mentioned in What is Arm Forge ). Arm forge describes this way of working as \u201cReverse connect\u201d in which we generate the profiles offline (by usual job submission scripts) and instead of using -X flag to use a GUI, we install the software on our local system. Once we have the software on our local system, we connect it with the one present on the server (DIaL3 in this case). Please follow the steps below to install Arm forge and connect it with the server. (A). First download the Arm forge from the official website (https://developer.arm.com/tools-and-software/server-and-hpc/downloads/arm-forge) (B). Install the arm forge in your desired location. For Linux, the downloaded files contain a file called \u2018installer\u2019. Double clicking on it will install the software in your desired location. (C). Change to the directory where you installed the software. For example, I installed it in /home/my_preferred_location/arm/forge/21.0.3. $ cd /home/my_preferred_location/arm/forge/21.0.3. (D). . Change directory to the bin directory. $ cd /home/my_preferred_location/arm/forge/21.0.3. (E). . Launch the arm forge software. $ ./forge (F). Go to the remote launch option in the main GUI and click configure. (G). After clicking configure, you will be presented to add a new connection. Click on add and fill in the following values (Please adjust accordingly). Connection Name: Fill in any name you want (e.g. Dial3) Host name:- Enter the host name to which you want to connect to. Arm forge expects these values to be a space separated list. (For dial, you will only need to enter the first value i.e. ignore the value after space). Eg. :- dc-ragt1@dial.dirac.ac.uk dc-ragt1@d3-login02.rcs.le.ac.uk Remote Installation Directory:- /cm/shared/apps/arm/forge/21.0.2 (Please do not change this). (H). Click Ok, then close. Now you will see an entry with the Connection name (here Dial3) in the main GUI under remote launch. Please click on this name and you will be prompted to enter your password twice. First to connect with Dial2.5 and then to Dial3. Please note that this is a one time setup and you do not need to do it again. You will only need to connect with the server anytime you want to use the Arm forge.","title":"Install Arm Forge"},{"location":"Analyse_code/Install_arm_forge/#install-arm-forge-on-your-pc","text":"You can install Arm forge on your local system and use the locally installed Arm forge to view your code profile. This method will be faster that X forwarding (as mentioned in What is Arm Forge ). Arm forge describes this way of working as \u201cReverse connect\u201d in which we generate the profiles offline (by usual job submission scripts) and instead of using -X flag to use a GUI, we install the software on our local system. Once we have the software on our local system, we connect it with the one present on the server (DIaL3 in this case). Please follow the steps below to install Arm forge and connect it with the server. (A). First download the Arm forge from the official website (https://developer.arm.com/tools-and-software/server-and-hpc/downloads/arm-forge) (B). Install the arm forge in your desired location. For Linux, the downloaded files contain a file called \u2018installer\u2019. Double clicking on it will install the software in your desired location. (C). Change to the directory where you installed the software. For example, I installed it in /home/my_preferred_location/arm/forge/21.0.3. $ cd /home/my_preferred_location/arm/forge/21.0.3. (D). . Change directory to the bin directory. $ cd /home/my_preferred_location/arm/forge/21.0.3. (E). . Launch the arm forge software. $ ./forge (F). Go to the remote launch option in the main GUI and click configure. (G). After clicking configure, you will be presented to add a new connection. Click on add and fill in the following values (Please adjust accordingly). Connection Name: Fill in any name you want (e.g. Dial3) Host name:- Enter the host name to which you want to connect to. Arm forge expects these values to be a space separated list. (For dial, you will only need to enter the first value i.e. ignore the value after space). Eg. :- dc-ragt1@dial.dirac.ac.uk dc-ragt1@d3-login02.rcs.le.ac.uk Remote Installation Directory:- /cm/shared/apps/arm/forge/21.0.2 (Please do not change this). (H). Click Ok, then close. Now you will see an entry with the Connection name (here Dial3) in the main GUI under remote launch. Please click on this name and you will be prompted to enter your password twice. First to connect with Dial2.5 and then to Dial3. Please note that this is a one time setup and you do not need to do it again. You will only need to connect with the server anytime you want to use the Arm forge.","title":"Install Arm forge on your PC"},{"location":"Analyse_code/Perf_report/","text":"Arm Performance Reports ARM Performance Reports is a low overhead tool that provides a high-level overview of your application\u2019s performance including computation, communication, and I/O. This tool provides a one-page text or HTML summary about the application's performance. To use this tool, all you need to do is prefix your execution command with perf-report . Please be advised that you do not need to recompile your code with any additional flags for generating a performance report. Instead, use the flags which you believe gives the best performance for your code. You can use the following command for this purpose. #For a single core Job. $ perf-report <executable> #For a MPI job. $ perf-report mpirun -n 4 <executable> You can also submit a job to scheduler to automatically generate the performance report for you. You can use the following example script (for an Hybrid MPI + OpenMP job) to generate the performance report for your code. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path perf-report --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable This will generate a .html file and .txt file which will show the various details about your code such as total run time, time consumed in MPI, whether code is compute bound or I/O bound etc. You can open the .html file in any browser. A sample image of performance report is shown below for your reference.","title":"Performance Report"},{"location":"Analyse_code/Perf_report/#arm-performance-reports","text":"ARM Performance Reports is a low overhead tool that provides a high-level overview of your application\u2019s performance including computation, communication, and I/O. This tool provides a one-page text or HTML summary about the application's performance. To use this tool, all you need to do is prefix your execution command with perf-report . Please be advised that you do not need to recompile your code with any additional flags for generating a performance report. Instead, use the flags which you believe gives the best performance for your code. You can use the following command for this purpose. #For a single core Job. $ perf-report <executable> #For a MPI job. $ perf-report mpirun -n 4 <executable> You can also submit a job to scheduler to automatically generate the performance report for you. You can use the following example script (for an Hybrid MPI + OpenMP job) to generate the performance report for your code. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path perf-report --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable This will generate a .html file and .txt file which will show the various details about your code such as total run time, time consumed in MPI, whether code is compute bound or I/O bound etc. You can open the .html file in any browser. A sample image of performance report is shown below for your reference.","title":"Arm Performance Reports"},{"location":"Analyse_code/Profile_code/","text":"Profile your code using Arm MAP Arm MAP is a Arm Forge's scalable profiler that can be used to profile serial and parallel applications. It allows you to profile your code without distorting application behaviour. It can be used to profile C, C++, Cuda, Fortran and Python code with no instrumentation or code changes. It helps the developers to accelerate their code by revealing the bottlenecks in their code such as the function taking maximum time, excessive time spent in MPI communication, slow I/O etc. In order to profile your code, please follow the stpes given below. 1. Recompile your code by including the debug flag i.e. -g . This will allow the Arm MAP to gather the necessary information during runtime. 2. You can use the following script to the scheduler which will automatically generate your code profile. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path #The following command is responsible for generating your code profile. map --profile --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable 3. Once your job ends, you will see a file with an extension .map which is your code profile. This file would be placed in the same directory from which the job was launched. 4. Now connect the arm forge form your local system to the server and click on Arm map in the main GUI as shown in the image below. 5. Click the option 'load the profile data from a previous run\u2019. Browse to the location of file generated in Step3 . 6. This will now open the map profile and will give all necessary details about your code such as: OpenMp stacks, function times, MPI times, overhead etc. We can navigate through the source code also to see which function is taking more time. A typical code profile would like the image shown below.","title":"Profile your Code"},{"location":"Analyse_code/Profile_code/#profile-your-code-using-arm-map","text":"Arm MAP is a Arm Forge's scalable profiler that can be used to profile serial and parallel applications. It allows you to profile your code without distorting application behaviour. It can be used to profile C, C++, Cuda, Fortran and Python code with no instrumentation or code changes. It helps the developers to accelerate their code by revealing the bottlenecks in their code such as the function taking maximum time, excessive time spent in MPI communication, slow I/O etc. In order to profile your code, please follow the stpes given below. 1. Recompile your code by including the debug flag i.e. -g . This will allow the Arm MAP to gather the necessary information during runtime. 2. You can use the following script to the scheduler which will automatically generate your code profile. #!/bin/bash --login #SBATCH -J Your_Job_Name #SBATCH -o Your_Object_File_Name.o%j #SBATCH -e Your_Error_File_Name.e%j #SBATCH -p Partition_on_which_to_run_code ##SBATCH -A Account #SBATCH --nodes=8 #SBATCH --ntasks=16 #SBATCH --ntasks-per-node=2 #SBATCH --exclusive #SBATCH -t HH:MM:SS export nodecnt=$SLURM_JOB_NUM_NODES export corecnt=`expr ${SLURM_CPUS_ON_NODE} \\* ${nodecnt}` export mpicnt=$SLURM_NTASKS export threadspermpi=`expr ${SLURM_CPUS_ON_NODE} \\/ ${SLURM_NTASKS_PER_NODE}` export threadcnt=`expr ${mpicnt} \\* ${threadspermpi}` export OMP_NUM_THREADS=$threadspermpi export OMP_PLACES=cores if [ $threadcnt -ne $corecnt ] then echo \"Error, mismatch between requested and available hardware!\" exit -1 fi #Dial3 module purge module load intel-parallel-studio/cluster.2019.5 module load arm/forge/21.0.2 #This will prevent Arm forge from closing if it cannot find license within a specified limit. export ALLINEA_NO_TIMEOUT=1 export EXE_DIR=Your_executable_directory_path #The following command is responsible for generating your code profile. map --profile --processes=$SLURM_NTASKS --procs-per-node $SLURM_NTASKS_PER_NODE --mpi=intel-mpi $EXE_DIR/Your_executable 3. Once your job ends, you will see a file with an extension .map which is your code profile. This file would be placed in the same directory from which the job was launched. 4. Now connect the arm forge form your local system to the server and click on Arm map in the main GUI as shown in the image below. 5. Click the option 'load the profile data from a previous run\u2019. Browse to the location of file generated in Step3 . 6. This will now open the map profile and will give all necessary details about your code such as: OpenMp stacks, function times, MPI times, overhead etc. We can navigate through the source code also to see which function is taking more time. A typical code profile would like the image shown below.","title":"Profile your code using Arm MAP"},{"location":"Analyse_code/about_arm_forge/","text":"Arm Forge Arm forge is one of the leading HPC and server development tools used in research, academia and industry for high performance codes written either in C, C++, Fortran or Python. Arm forge has three major components:- Arm Performance Reports Arm MAP Arm DDT To use Arm forge on DiAL3, please load the Arm forge module by the following command. module load arm/forge/21.0.2 . You can use Arm forge in two different ways:- Using X forwarding (might be slow) By installing Arm forge on your local computer (No License needed). Use Arm Forge by X forwarding First login to DiAL3 cluster by using the following command (please pay attention to the -X flag in the command). $ ssh -X your_username@dial3.dirac.ac.uk Enter your password to login. Once you have logged in, load the Arm forge module by entering the following command in the terminal. $ module load arm/forge/21.0.2 To see what all paths and variables have been added to your environment by the above load command, type the following. $ module display arm/forge/21.0.2 This will show the following output on your screen. ---------------------------------------------------------------------------- /cm/shared/modulefiles/arm/forge/21.0.2: ---------------------------------------------------------------------------- whatis(\"Loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment \") prepend_path(\"PATH\",\"/cm/shared/apps/arm/forge/21.0.2/bin\") prepend_path(\"LIBRARY_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"LD_RUN_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"CPATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") setenv(\"ARM_LICENSE_DIR\",\"/cm/shared/apps/arm/forge/21.0.2/licences\") help([[ forge/21.0.2 - loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment ]]) If you see the first entry in the prepend path, you will see the location where all binaries of arm forge are located. Change to that directory to see what all binaries are there. $ cd /cm/shared/apps/arm/forge/21.0.2/bin #List the contents of that directory. $ ls allinea-client ddt-mpirun forge-mpirun map ddt forge forge-probe perf-report ddt-client forge-client make-profiler-libraries #Open the forge GUI by using the following command. $ ./forge Now you can use this GUI to debug your code or view profile reports . USE Arm forge by Installing on your PC You can also use Arm forge by first installing it on your PC (you do not need a license for that) and using that to view your code profile. For more details, please click Install Arm Forge .","title":"What is Arm Forge"},{"location":"Analyse_code/about_arm_forge/#arm-forge","text":"Arm forge is one of the leading HPC and server development tools used in research, academia and industry for high performance codes written either in C, C++, Fortran or Python. Arm forge has three major components:- Arm Performance Reports Arm MAP Arm DDT To use Arm forge on DiAL3, please load the Arm forge module by the following command. module load arm/forge/21.0.2 . You can use Arm forge in two different ways:- Using X forwarding (might be slow) By installing Arm forge on your local computer (No License needed).","title":"Arm Forge"},{"location":"Analyse_code/about_arm_forge/#use-arm-forge-by-x-forwarding","text":"First login to DiAL3 cluster by using the following command (please pay attention to the -X flag in the command). $ ssh -X your_username@dial3.dirac.ac.uk Enter your password to login. Once you have logged in, load the Arm forge module by entering the following command in the terminal. $ module load arm/forge/21.0.2 To see what all paths and variables have been added to your environment by the above load command, type the following. $ module display arm/forge/21.0.2 This will show the following output on your screen. ---------------------------------------------------------------------------- /cm/shared/modulefiles/arm/forge/21.0.2: ---------------------------------------------------------------------------- whatis(\"Loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment \") prepend_path(\"PATH\",\"/cm/shared/apps/arm/forge/21.0.2/bin\") prepend_path(\"LIBRARY_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"LD_RUN_PATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") prepend_path(\"CPATH\",\"/cm/shared/apps/arm/forge/21.0.2/lib\") setenv(\"ARM_LICENSE_DIR\",\"/cm/shared/apps/arm/forge/21.0.2/licences\") help([[ forge/21.0.2 - loads the ARM Forge - MPI profiler and debugger (21.0.2) software environment ]]) If you see the first entry in the prepend path, you will see the location where all binaries of arm forge are located. Change to that directory to see what all binaries are there. $ cd /cm/shared/apps/arm/forge/21.0.2/bin #List the contents of that directory. $ ls allinea-client ddt-mpirun forge-mpirun map ddt forge forge-probe perf-report ddt-client forge-client make-profiler-libraries #Open the forge GUI by using the following command. $ ./forge Now you can use this GUI to debug your code or view profile reports .","title":"Use Arm Forge by X forwarding"},{"location":"Analyse_code/about_arm_forge/#use-arm-forge-by-installing-on-your-pc","text":"You can also use Arm forge by first installing it on your PC (you do not need a license for that) and using that to view your code profile. For more details, please click Install Arm Forge .","title":"USE Arm forge by Installing on your PC"},{"location":"Getting_started/connecting_dial3/","text":"Connecting to DIaL3 Once you have requested acsess to DIaL3 , you can access the login nodes from an SSH client using either of the following address. dial3.dirac.ac.uk , d3-login01.rcs.le.ac.uk or d3-login02.rcs.le.ac.uk From your terminal type the following command: $ ssh your_username@d3-login02.rcs.le.ac.uk You will be prompted to enter your password. $ your_username@d3-login02.rcs.le.ac.uk's password: Once you enter your password, you will see the following message on your screen. Last login: Sat Sep 4 10:27:27 2021 from 444.222.111.88 # The above message shows the last time you logged in and the IP address from which you logged in.","title":"Connecting"},{"location":"Getting_started/connecting_dial3/#connecting-to-dial3","text":"Once you have requested acsess to DIaL3 , you can access the login nodes from an SSH client using either of the following address. dial3.dirac.ac.uk , d3-login01.rcs.le.ac.uk or d3-login02.rcs.le.ac.uk From your terminal type the following command: $ ssh your_username@d3-login02.rcs.le.ac.uk You will be prompted to enter your password. $ your_username@d3-login02.rcs.le.ac.uk's password: Once you enter your password, you will see the following message on your screen. Last login: Sat Sep 4 10:27:27 2021 from 444.222.111.88 # The above message shows the last time you logged in and the IP address from which you logged in.","title":"Connecting to DIaL3"},{"location":"Getting_started/create_account/","text":"Request Access In order to use DIaL3 system at the University of Leicester, you need to apply for an account. Access to the DIaL3 (part of DiRAC) service is through peer reviewed applications. There are details of how to request access on the DiRAC website. Once access has been approved, accounts are requested through the SAFE project and user management system operated by EPCC . SAFE stands for S ervice A dministration F rom E PCC. Every DiRAC user has an account on SAFE. You can find more information on the DiRAC website .","title":"Create Account"},{"location":"Getting_started/create_account/#request-access","text":"In order to use DIaL3 system at the University of Leicester, you need to apply for an account. Access to the DIaL3 (part of DiRAC) service is through peer reviewed applications. There are details of how to request access on the DiRAC website. Once access has been approved, accounts are requested through the SAFE project and user management system operated by EPCC . SAFE stands for S ervice A dministration F rom E PCC. Every DiRAC user has an account on SAFE. You can find more information on the DiRAC website .","title":"Request Access"},{"location":"Run_computation/Scheduler/","text":"","title":"Scheduler"},{"location":"Run_computation/interactive_or_batch/","text":"","title":"Intercative or batch"},{"location":"Run_computation/job_types/","text":"","title":"Job types"},{"location":"Run_computation/monitor_jobs/","text":"","title":"Monitor jobs"},{"location":"Run_computation/submit_job/","text":"","title":"Submit a Job"},{"location":"Support/Contact/","text":"Contact If you need any assitance in using DIaL3 services, please send a suport request to leicester.support@dirac.ac.uk , and our team will try to help you as soon as possible. In your Email, please share as much detail as possible about your query. Image Credit: https://xkcd.com/293/","title":"Contact"},{"location":"Support/Contact/#contact","text":"If you need any assitance in using DIaL3 services, please send a suport request to leicester.support@dirac.ac.uk , and our team will try to help you as soon as possible. In your Email, please share as much detail as possible about your query. Image Credit: https://xkcd.com/293/","title":"Contact"},{"location":"Support/FAQ/","text":"How do I check my disk usage / quota? How do I check my allocation?","title":"Frequently asked questions"},{"location":"Support/FAQ/#how-do-i-check-my-disk-usage-quota","text":"","title":"How do I check my disk usage / quota?"},{"location":"Support/FAQ/#how-do-i-check-my-allocation","text":"","title":"How do I check my allocation?"},{"location":"Support/Support_staff/","text":"Meet our Team The support team for DIaL3 consists of the following members. JonWakelin :- Team lead at ITS Liam :- Gary :- Chris :- Makis Kappas :- Research Software Engineer Lokesh Ragta :- Research Software Engineer","title":"Support Staff"},{"location":"Support/Support_staff/#meet-our-team","text":"The support team for DIaL3 consists of the following members. JonWakelin :- Team lead at ITS Liam :- Gary :- Chris :- Makis Kappas :- Research Software Engineer Lokesh Ragta :- Research Software Engineer","title":"Meet our Team"}]}